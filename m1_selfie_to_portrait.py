#!/usr/bin/env python3
"""
ğŸ M1 ìµœì í™” ì…€ì¹´â†’ì´ˆìƒí™” ë³€í™˜ê¸°
Apple Silicon M1/M2ì— ìµœì í™”ëœ ë²„ì „ì…ë‹ˆë‹¤.
"""

import torch
import argparse
import os
import cv2
import numpy as np
from PIL import Image
from diffusers import StableDiffusionImg2ImgPipeline
import mediapipe as mp

class M1OptimizedSelfieToPortrait:
    def __init__(self, lora_model_path="lora_trained_model/final"):
        """M1 ìµœì í™” ì…€ì¹´â†’ì´ˆìƒí™” ë³€í™˜ê¸° ì´ˆê¸°í™”"""
        print("ğŸ M1 ìµœì í™” ì…€ì¹´â†’ì´ˆìƒí™” ë³€í™˜ê¸° ì´ˆê¸°í™” ì¤‘...")
        
        # M1/M2 GPU ì‚¬ìš© ì„¤ì •
        if torch.backends.mps.is_available():
            self.device = "mps"  # Apple Silicon GPU
            print("ğŸš€ Apple Silicon GPU (MPS) ì‚¬ìš©!")
        elif torch.cuda.is_available():
            self.device = "cuda"
            print("ğŸš€ NVIDIA GPU ì‚¬ìš©!")
        else:
            self.device = "cpu"
            print("ğŸ’» CPU ì‚¬ìš©")
        
        print(f"ğŸ“± ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        # MediaPipe Face Detection ì´ˆê¸°í™”
        print("ğŸ‘¤ ì–¼êµ´ ê²€ì¶œ ëª¨ë¸ ë¡œë“œ ì¤‘...")
        self.mp_face_detection = mp.solutions.face_detection
        self.face_detection = self.mp_face_detection.FaceDetection(
            model_selection=1, min_detection_confidence=0.5
        )
        
        print("âœ… M1 ìµœì í™” ë³€í™˜ê¸° ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def detect_face(self, image_path):
        """ì´ë¯¸ì§€ì—ì„œ ì–¼êµ´ ê²€ì¶œ"""
        print(f"ğŸ‘¤ ì–¼êµ´ ê²€ì¶œ ì¤‘: {image_path}")
        
        # ì´ë¯¸ì§€ ì½ê¸°
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"ì´ë¯¸ì§€ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
        
        # RGBë¡œ ë³€í™˜
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # ì–¼êµ´ ê²€ì¶œ
        results = self.face_detection.process(image_rgb)
        
        if not results.detections:
            raise ValueError("ì–¼êµ´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        
        print(f"âœ… {len(results.detections)}ê°œì˜ ì–¼êµ´ ë°œê²¬!")
        return image, image_rgb, results.detections
    
    def analyze_emotion_from_image(self, image_path):
        """ì´ë¯¸ì§€ì—ì„œ ê°ì • ë¶„ì„ (ê°„ë‹¨í•œ ë°©ë²•)"""
        print("ğŸ” ì´ë¯¸ì§€ ê°ì • ë¶„ì„ ì¤‘...")
        
        try:
            # ì–¼êµ´ ê²€ì¶œ
            image, image_rgb, detections = self.detect_face(image_path)
            
            # ì²« ë²ˆì§¸ ì–¼êµ´ ë¶„ì„
            face = detections[0]
            bbox = face.location_data.relative_bounding_box
            
            # ì–¼êµ´ ì˜ì—­ ì¶”ì¶œ
            h, w = image_rgb.shape[:2]
            x = int(bbox.xmin * w)
            y = int(bbox.ymin * h)
            width = int(bbox.width * w)
            height = int(bbox.height * h)
            
            face_crop = image_rgb[y:y+height, x:x+width]
            
            # ê°„ë‹¨í•œ ìƒ‰ìƒ ë¶„ì„ìœ¼ë¡œ ê°ì • ì¶”ì •
            avg_color = np.mean(face_crop, axis=(0, 1))
            
            # ë°ê¸° ê¸°ë°˜ ê°ì • ì¶”ì •
            brightness = np.mean(avg_color)
            
            if brightness > 150:  # ë°ì€ ì´ë¯¸ì§€
                emotion = "joy"
            elif brightness < 100:  # ì–´ë‘ìš´ ì´ë¯¸ì§€
                emotion = "sadness"
            else:
                emotion = "neutral"
            
            print(f"ğŸ’­ ë¶„ì„ëœ ê°ì •: {emotion} (ë°ê¸°: {brightness:.1f})")
            return emotion, {emotion: 0.8}
            
        except Exception as e:
            print(f"âš ï¸ ê°ì • ë¶„ì„ ì‹¤íŒ¨: {e}")
            print("ğŸ’¡ ê¸°ë³¸ê°’ 'joy' ì‚¬ìš©")
            return "joy", {"joy": 0.8}
    
    def get_emotion_prompt(self, emotion):
        """ê°ì •ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        emotion_prompts = {
            'joy': "a joyful portrait, happy expression, warm smile, bright eyes, cheerful, beautiful lighting, vibrant colors, masterpiece",
            'sadness': "a melancholic portrait, sad expression, contemplative gaze, gentle mood, soft lighting, muted colors, artistic",
            'anger': "a powerful portrait, intense expression, strong features, dramatic lighting, bold colors, dynamic",
            'fear': "a thoughtful portrait, cautious expression, mysterious atmosphere, subtle lighting, cool tones, introspective",
            'surprise': "an expressive portrait, surprised expression, wide eyes, dynamic pose, bright lighting, energetic",
            'disgust': "a dignified portrait, composed expression, refined features, elegant lighting, sophisticated, classical",
            'neutral': "a serene portrait, calm expression, peaceful atmosphere, balanced lighting, natural colors, tranquil",
            'love': "a tender portrait, gentle expression, warm atmosphere, caring eyes, romantic lighting, soft colors, intimate"
        }
        return emotion_prompts.get(emotion, emotion_prompts['joy'])
    
    def create_portrait_m1(self, image_path, output_path="my_portrait_m1.png"):
        """
        M1 ìµœì í™” ì´ˆìƒí™” ìƒì„±
        """
        print(f"\nğŸ¨ M1 ìµœì í™” ì´ˆìƒí™” ìƒì„± ì‹œì‘!")
        print(f"ğŸ“¸ ì…ë ¥ ì´ë¯¸ì§€: {image_path}")
        
        # 1. ê°ì • ë¶„ì„
        emotion, emotion_scores = self.analyze_emotion_from_image(image_path)
        
        # 2. í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self.get_emotion_prompt(emotion)
        print(f"ğŸ“ ìƒì„± í”„ë¡¬í”„íŠ¸: {prompt}")
        
        # 3. M1 ìµœì í™” íŒŒì´í”„ë¼ì¸ ë¡œë“œ
        print("ğŸ¨ M1 ìµœì í™” íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì¤‘...")
        
        # íŒŒì´í”„ë¼ì¸ ë¡œë“œ (M1 ìµœì í™”)
        pipe = StableDiffusionImg2ImgPipeline.from_pretrained(
            "runwayml/stable-diffusion-v1-5",
            torch_dtype=torch.float16 if self.device in ["mps", "cuda"] else torch.float32,
            safety_checker=None,
            requires_safety_checker=False
        )
        
        # LoRA ëª¨ë¸ ë¡œë“œ
        lora_path = "lora_trained_model/final"
        if os.path.exists(lora_path):
            print(f"ğŸ­ LoRA ëª¨ë¸ ë¡œë“œ: {lora_path}")
            pipe.load_lora_weights(lora_path)
        
        # M1 GPUë¡œ ì´ë™
        if self.device == "mps":
            pipe = pipe.to(self.device)
            print("ğŸš€ Apple Silicon GPUë¡œ ì´ë™ ì™„ë£Œ!")
        elif self.device == "cuda":
            pipe = pipe.to(self.device)
            print("ğŸš€ NVIDIA GPUë¡œ ì´ë™ ì™„ë£Œ!")
        
        # ì›ë³¸ ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬
        init_image = Image.open(image_path).convert("RGB")
        init_image = init_image.resize((512, 512))
        
        # ì´ë¯¸ì§€ ìƒì„± (M1 ìµœì í™” ì„¤ì •)
        print("ğŸ–¼ï¸ ì´ë¯¸ì§€ ìƒì„± ì¤‘... (M1 GPU ê°€ì† ì‚¬ìš©)")
        result = pipe(
            prompt=prompt,
            image=init_image,
            strength=0.7,  # ì›ë³¸ ì–¼êµ´ êµ¬ì¡° ìœ ì§€
            guidance_scale=7.5,
            num_inference_steps=20,  # ë¹ ë¥¸ ìƒì„±
            generator=torch.Generator(device=self.device)  # M1 ìµœì í™”
        )
        
        # ì €ì¥
        result.images[0].save(output_path)
        print(f"âœ… ì´ˆìƒí™” ì €ì¥ ì™„ë£Œ: {output_path}")
        
        return result.images[0], emotion, emotion_scores


def main():
    parser = argparse.ArgumentParser(description="M1 ìµœì í™” ì…€ì¹´â†’ì´ˆìƒí™” ë³€í™˜")
    parser.add_argument("--input", type=str, required=True, help="ì…ë ¥ ì…€ì¹´ ì´ë¯¸ì§€ ê²½ë¡œ")
    parser.add_argument("--output", type=str, default="my_portrait_m1.png", help="ì¶œë ¥ íŒŒì¼ëª…")
    parser.add_argument("--lora-path", type=str, default="lora_trained_model/final", 
                       help="LoRA ëª¨ë¸ ê²½ë¡œ")
    
    args = parser.parse_args()
    
    # ì…ë ¥ íŒŒì¼ í™•ì¸
    if not os.path.exists(args.input):
        print(f"âŒ ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.input}")
        return
    
    # ë³€í™˜ê¸° ì´ˆê¸°í™”
    converter = M1OptimizedSelfieToPortrait(lora_model_path=args.lora_path)
    
    # ì´ˆìƒí™” ìƒì„±
    image, emotion, scores = converter.create_portrait_m1(
        args.input, 
        args.output
    )
    
    print(f"\nğŸ‰ ì™„ë£Œ!")
    print(f"ğŸ­ ê°ì •: {emotion}")
    print(f"ğŸ“Š ê°ì • ì ìˆ˜: {scores}")
    print(f"ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {args.output}")

if __name__ == "__main__":
    main()





