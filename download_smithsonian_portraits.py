#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ìŠ¤ë¯¸ì†Œë‹ˆì–¸ êµ­ë¦½ ì´ˆìƒí™” ê°¤ëŸ¬ë¦¬ì—ì„œ ì´ˆìƒí™” ë‹¤ìš´ë¡œë“œ
100% ì´ˆìƒí™” ë³´ì¥, ê³ í’ˆì§ˆ ì´ë¯¸ì§€
"""

import requests
import os
import time
import random
import re
import json
from urllib.parse import urljoin, urlparse
import pandas as pd
from datetime import datetime

# ì„¤ì •
SAVE_DIR = "data/smithsonian_portraits"
METADATA_DIR = "data/smithsonian_metadata"
os.makedirs(SAVE_DIR, exist_ok=True)
os.makedirs(METADATA_DIR, exist_ok=True)

# ìŠ¤ë¯¸ì†Œë‹ˆì–¸ API ì—”ë“œí¬ì¸íŠ¸ë“¤
SMITHSONIAN_BASE_URL = "https://api.si.edu/openaccess/api/v1.0"
SMITHSONIAN_SEARCH_URL = f"{SMITHSONIAN_BASE_URL}/search"
SMITHSONIAN_OBJECT_URL = f"{SMITHSONIAN_BASE_URL}/content"

# User-Agent ëª©ë¡
USER_AGENTS = [
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0"
]

def get_random_headers():
    """ëœë¤ í—¤ë” ìƒì„±"""
    return {
        "User-Agent": random.choice(USER_AGENTS),
        "Accept": "application/json,text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5",
        "Accept-Encoding": "gzip, deflate",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
    }

def human_delay():
    """ì¸ê°„ì ì¸ ì§€ì—°"""
    delay = random.uniform(1.0, 2.5)
    time.sleep(delay)

def search_smithsonian_portraits(limit=1000):
    """ìŠ¤ë¯¸ì†Œë‹ˆì–¸ì—ì„œ ì´ˆìƒí™” ê²€ìƒ‰"""
    print(f"ğŸ” ìŠ¤ë¯¸ì†Œë‹ˆì–¸ êµ­ë¦½ ì´ˆìƒí™” ê°¤ëŸ¬ë¦¬ì—ì„œ ì´ˆìƒí™” ê²€ìƒ‰ ì¤‘... (ëª©í‘œ: {limit}ê°œ)")
    
    all_objects = []
    start = 0
    rows = 100  # í•œ ë²ˆì— ê°€ì ¸ì˜¬ ìˆ˜ ìˆëŠ” ìµœëŒ€ ê°œìˆ˜
    
    session = requests.Session()
    
    while len(all_objects) < limit:
        try:
            # ê²€ìƒ‰ íŒŒë¼ë¯¸í„°
            params = {
                "q": "portrait",  # ì´ˆìƒí™” ê²€ìƒ‰
                "start": start,
                "rows": min(rows, limit - len(all_objects)),
                "fq": "unit_code:NPG",  # National Portrait Galleryë§Œ
                # "api_key": "YOUR_API_KEY"  # API í‚¤ ì—†ì´ë„ ì ‘ê·¼ ê°€ëŠ¥
            }
            
            print(f"  ğŸ“¡ í˜ì´ì§€ {start//rows + 1} ìš”ì²­ ì¤‘... (start={start})")
            
            response = session.get(
                SMITHSONIAN_SEARCH_URL,
                params=params,
                headers=get_random_headers(),
                timeout=30
            )
            
            if response.status_code == 200:
                data = response.json()
                objects = data.get("response", {}).get("docs", [])
                
                if not objects:
                    print("  âœ… ë” ì´ìƒ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    break
                
                all_objects.extend(objects)
                print(f"  âœ… {len(objects)}ê°œ ë°œê²¬ (ì´ {len(all_objects)}ê°œ)")
                
                start += rows
                human_delay()
                
            else:
                print(f"  âŒ API ì˜¤ë¥˜: {response.status_code}")
                if response.status_code == 429:  # Rate limit
                    print("  ğŸ˜´ Rate limit - 60ì´ˆ ëŒ€ê¸°...")
                    time.sleep(60)
                else:
                    break
                    
        except Exception as e:
            print(f"  âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            break
    
    print(f"ğŸ‰ ì´ {len(all_objects)}ê°œì˜ ì´ˆìƒí™” ì‘í’ˆ ë°œê²¬!")
    return all_objects[:limit]

def get_object_details(object_id, session):
    """ì‘í’ˆ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
    try:
        url = f"{SMITHSONIAN_OBJECT_URL}/{object_id}"
        response = session.get(url, headers=get_random_headers(), timeout=20)
        
        if response.status_code == 200:
            return response.json()
        else:
            print(f"    âŒ ìƒì„¸ ì •ë³´ ì˜¤ë¥˜: {response.status_code}")
            return None
    except Exception as e:
        print(f"    âŒ ìƒì„¸ ì •ë³´ ì˜ˆì™¸: {e}")
        return None

def download_image(image_url, filename, session):
    """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ"""
    try:
        response = session.get(image_url, headers=get_random_headers(), timeout=30)
        
        if response.status_code == 200 and response.content:
            filepath = os.path.join(SAVE_DIR, filename)
            with open(filepath, "wb") as f:
                f.write(response.content)
            
            file_size_mb = len(response.content) / (1024 * 1024)
            return True, file_size_mb
        else:
            return False, 0
    except Exception as e:
        print(f"    âŒ ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
        return False, 0

def main():
    print("ğŸ›ï¸ === ìŠ¤ë¯¸ì†Œë‹ˆì–¸ êµ­ë¦½ ì´ˆìƒí™” ê°¤ëŸ¬ë¦¬ ë‹¤ìš´ë¡œë” ===")
    print("ğŸ“‹ 100% ì´ˆìƒí™” ë³´ì¥, ê³ í’ˆì§ˆ ì´ë¯¸ì§€")
    print("=" * 60)
    
    # 1ë‹¨ê³„: ì´ˆìƒí™” ê²€ìƒ‰
    all_objects = search_smithsonian_portraits(limit=1000)
    
    if not all_objects:
        print("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 2ë‹¨ê³„: ë‹¤ìš´ë¡œë“œ
    print(f"\nğŸ“¥ {len(all_objects)}ê°œ ì‘í’ˆ ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
    
    session = requests.Session()
    results = []
    downloaded_count = 0
    failed_count = 0
    
    for i, obj in enumerate(all_objects):
        object_id = obj.get("id", "")
        title = obj.get("title", "Unknown")
        
        print(f"[{i+1}/{len(all_objects)}] ë¶„ì„ ì¤‘: ID {object_id}")
        print(f"  ğŸ¨ {title}")
        
        # ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        details = get_object_details(object_id, session)
        if not details:
            print(f"  âŒ ìƒì„¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ")
            failed_count += 1
            results.append({
                "object_id": object_id,
                "title": title,
                "status": "failed",
                "reason": "ìƒì„¸ ì •ë³´ ì—†ìŒ"
            })
            continue
        
        # ì´ë¯¸ì§€ URL ì°¾ê¸°
        image_url = None
        if "indexedStructuredData" in details:
            structured_data = details["indexedStructuredData"]
            if "descriptiveNonRepeating" in structured_data:
                desc = structured_data["descriptiveNonRepeating"]
                if "online_media" in desc and "media" in desc["online_media"]:
                    media = desc["online_media"]["media"]
                    if media and len(media) > 0:
                        image_url = media[0].get("content", "")
        
        if not image_url:
            print(f"  âŒ ì´ë¯¸ì§€ URL ì—†ìŒ")
            failed_count += 1
            results.append({
                "object_id": object_id,
                "title": title,
                "status": "failed",
                "reason": "ì´ë¯¸ì§€ URL ì—†ìŒ"
            })
            continue
        
        # íŒŒì¼ëª… ìƒì„±
        safe_title = re.sub(r"[^\w\-\.]+", "_", title)[:50]
        filename = f"{object_id}_{safe_title}.jpg"
        
        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
        success, file_size = download_image(image_url, filename, session)
        
        if success:
            print(f"  âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {filename} ({file_size:.1f}MB)")
            downloaded_count += 1
            results.append({
                "object_id": object_id,
                "title": title,
                "filename": filename,
                "file_size_mb": file_size,
                "status": "success"
            })
        else:
            print(f"  âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
            failed_count += 1
            results.append({
                "object_id": object_id,
                "title": title,
                "status": "failed",
                "reason": "ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨"
            })
        
        # ì§„í–‰ ìƒí™© í‘œì‹œ
        if (i + 1) % 50 == 0:
            print(f"\nğŸ“Š ì§„í–‰ ìƒí™©: {i+1}/{len(all_objects)} ì™„ë£Œ")
            print(f"  âœ… ì„±ê³µ: {downloaded_count}ê°œ")
            print(f"  âŒ ì‹¤íŒ¨: {failed_count}ê°œ")
            print(f"  ğŸ“ˆ ì„±ê³µë¥ : {(downloaded_count/(i+1)*100):.1f}%")
        
        human_delay()
    
    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_path = os.path.join(METADATA_DIR, f"smithsonian_portraits_{timestamp}.csv")
    
    df = pd.DataFrame(results)
    df.to_csv(csv_path, index=False, encoding='utf-8')
    
    # ìµœì¢… í†µê³„
    total_size = sum([r.get('file_size_mb', 0) for r in results if r.get('status') == 'success'])
    
    print(f"\nğŸ‰ === ìŠ¤ë¯¸ì†Œë‹ˆì–¸ ì´ˆìƒí™” ìˆ˜ì§‘ ì™„ë£Œ ===")
    print(f"âœ… ì´ ë‹¤ìš´ë¡œë“œ: {downloaded_count}ê°œ")
    print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {failed_count}ê°œ")
    print(f"ğŸ“ ì´ë¯¸ì§€ ì €ì¥: {SAVE_DIR}")
    print(f"ğŸ“„ ë©”íƒ€ë°ì´í„°: {csv_path}")
    print(f"ğŸ’¾ ì´ íŒŒì¼ í¬ê¸°: {total_size:.1f}MB")

if __name__ == "__main__":
    main()
