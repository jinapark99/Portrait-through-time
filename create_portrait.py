#!/usr/bin/env python3
"""
ğŸ¨ ê°„ë‹¨í•œ ì…€ì¹´ â†’ ì´ˆìƒí™” ë³€í™˜ê¸°
ì‚¬ì§„ì„ ë„£ìœ¼ë©´ ì´ˆìƒí™”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤!
"""

import torch
import argparse
import cv2
import numpy as np
from PIL import Image
from diffusers import StableDiffusionImg2ImgPipeline
import mediapipe as mp
import os

class SelfieToPortrait:
    def __init__(self):
        print("ğŸ¨ ì…€ì¹´ â†’ ì´ˆìƒí™” ë³€í™˜ê¸° ì´ˆê¸°í™” ì¤‘...")
        
        # M1 GPU ì„¤ì • (MPS ì§€ì›)
        if torch.backends.mps.is_available():
            self.device = "mps"
        elif torch.cuda.is_available():
            self.device = "cuda"
        else:
            self.device = "cpu"
        print(f"ğŸ“± ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        # MediaPipe ì–¼êµ´ ê°ì§€ ì´ˆê¸°í™”
        self.mp_face_detection = mp.solutions.face_detection
        self.face_detection = self.mp_face_detection.FaceDetection(
            model_selection=0, min_detection_confidence=0.5
        )
        
        # Stable Diffusion íŒŒì´í”„ë¼ì¸ ë¡œë“œ
        print("ğŸ¨ Stable Diffusion íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì¤‘... (ì²˜ìŒ ì‹¤í–‰ ì‹œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        self.pipe = StableDiffusionImg2ImgPipeline.from_pretrained(
            "runwayml/stable-diffusion-v1-5",
            torch_dtype=torch.float16 if self.device in ["mps", "cuda"] else torch.float32,
            safety_checker=None,
            requires_safety_checker=False
        )
        self.pipe = self.pipe.to(self.device)
        
        # LoRA ë¡œë“œ (ìˆëŠ” ê²½ìš°)
        lora_path = "lora_trained_model/final"
        if os.path.exists(lora_path):
            print("ğŸ­ LoRA ëª¨ë¸ ë¡œë“œ ì¤‘...")
            try:
                self.pipe.load_lora_weights(lora_path)
                print("âœ… LoRA ë¡œë“œ ì„±ê³µ!")
            except Exception as e:
                print(f"âš ï¸ LoRA ë¡œë“œ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")
        else:
            print("ğŸ’¡ LoRA ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë¸ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
        
        print("âœ… ì´ˆê¸°í™” ì™„ë£Œ!\n")
    
    def detect_face(self, image):
        """ì–¼êµ´ ê°ì§€"""
        # PILì„ OpenCV í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        
        # ì–¼êµ´ ê°ì§€
        results = self.face_detection.process(cv_image)
        
        if results.detections:
            # ì²« ë²ˆì§¸ ì–¼êµ´ì˜ ë°”ìš´ë”© ë°•ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            detection = results.detections[0]
            bbox = detection.location_data.relative_bounding_box
            
            h, w, _ = cv_image.shape
            x = int(bbox.xmin * w)
            y = int(bbox.ymin * h)
            width = int(bbox.width * w)
            height = int(bbox.height * h)
            
            # ì–¼êµ´ ì˜ì—­ í¬ë¡­ (ì•½ê°„ì˜ ì—¬ìœ  ê³µê°„ ì¶”ê°€)
            padding = 20
            x = max(0, x - padding)
            y = max(0, y - padding)
            width = min(w - x, width + padding * 2)
            height = min(h - y, height + padding * 2)
            
            face_crop = image.crop((x, y, x + width, y + height))
            
            print(f"âœ… ì–¼êµ´ ê°ì§€ ì„±ê³µ: {width}x{height}")
            return face_crop, True
        else:
            print("âš ï¸ ì–¼êµ´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return image, False
    
    def analyze_emotion_simple(self, face_image):
        """ê°„ë‹¨í•œ ê°ì • ë¶„ì„"""
        # ì–¼êµ´ ëœë“œë§ˆí¬ ê°ì§€
        mp_face_mesh = mp.solutions.face_mesh
        face_mesh = mp_face_mesh.FaceMesh(
            static_image_mode=True,
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.3,
            min_tracking_confidence=0.3
        )
        
        # PILì„ numpyë¡œ ë³€í™˜
        img_array = np.array(face_image)
        
        # ì–¼êµ´ ëœë“œë§ˆí¬ ê°ì§€
        results = face_mesh.process(img_array)
        
        if results.multi_face_landmarks:
            landmarks = results.multi_face_landmarks[0]
            
            # ì…ê¼¬ë¦¬ ì¢Œí‘œ (ì¸ë±ìŠ¤ 61, 291)
            left_corner = landmarks.landmark[61]
            right_corner = landmarks.landmark[291]
            
            # ì…ê¼¬ë¦¬ ë†’ì´ ì°¨ì´ ê³„ì‚°
            mouth_curve = right_corner.y - left_corner.y
            
            print(f"ğŸ” ì…ê¼¬ë¦¬ ê³¡ì„  ê°’: {mouth_curve:.4f}")
            
            # ê°„ë‹¨í•œ ê°ì • íŒë‹¨
            if mouth_curve < -0.005:  # ì…ê¼¬ë¦¬ê°€ ìœ„ë¡œ ì˜¬ë¼ê°
                emotion = "joyful"
                print("ğŸ˜Š ê°ì • ë¶„ì„: ê¸°ì¨ (ì›ƒëŠ” í‘œì •)")
            else:
                emotion = "neutral"
                print("ğŸ˜ ê°ì • ë¶„ì„: ì¤‘ë¦½")
        else:
            # ëœë“œë§ˆí¬ ê°ì§€ ì‹¤íŒ¨ ì‹œ ì´ë¯¸ì§€ ë°ê¸°ë¡œ ê°ì • ì¶”ì •
            brightness = np.mean(img_array)
            
            if brightness > 120:  # ë°ì€ ì´ë¯¸ì§€
                emotion = "joyful"
                print("ğŸ˜Š ê°ì • ë¶„ì„: ê¸°ì¨ (ë°ì€ ì´ë¯¸ì§€ë¡œ ì¶”ì •)")
            else:
                emotion = "neutral"
                print("ğŸ˜ ê°ì • ë¶„ì„: ì¤‘ë¦½ (ë°ê¸° ê¸°ë°˜)")
        
        return emotion
    
    def create_portrait(self, selfie_path, output_path=None, emotion_override=None):
        """ì…€ì¹´ë¥¼ í¬íŠ¸ë ˆì´íŠ¸ë¡œ ë³€í™˜"""
        print(f"\nğŸ¨ ì…€ì¹´ ì²˜ë¦¬ ì‹œì‘: {selfie_path}")
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        try:
            image = Image.open(selfie_path).convert("RGB")
            print(f"âœ… ì´ë¯¸ì§€ ë¡œë“œ ì„±ê³µ: {image.size}")
        except Exception as e:
            print(f"âŒ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
        
        # ì–¼êµ´ ê°ì§€
        face_crop, face_found = self.detect_face(image)
        
        # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • (512x512)
        input_image = face_crop.resize((512, 512))
        
        # ê°ì • ë¶„ì„ (ì˜¤ë²„ë¼ì´ë“œê°€ ì—†ìœ¼ë©´)
        if emotion_override:
            emotion = emotion_override
            print(f"ğŸ­ ì‚¬ìš©ì ì§€ì • ê°ì •: {emotion}")
        else:
            emotion = self.analyze_emotion_simple(input_image)
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = f"portrait painting, {emotion} expression, classical art style, high quality, detailed"
        print(f"ğŸ“ ì‚¬ìš© í”„ë¡¬í”„íŠ¸: {prompt}")
        
        # ì¶œë ¥ ê²½ë¡œ ì„¤ì •
        if output_path is None:
            base_name = os.path.splitext(os.path.basename(selfie_path))[0]
            output_path = f"portrait_{base_name}_{emotion}.png"
        
        # ì´ë¯¸ì§€ ìƒì„±
        print("ğŸ¨ ì´ë¯¸ì§€ ìƒì„± ì¤‘... (ì•½ 1-3ë¶„ ì†Œìš”, GPUì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        try:
            generator = torch.Generator(device=self.device)
            result = self.pipe(
                prompt=prompt,
                image=input_image,
                strength=0.7,  # ì›ë³¸ ì´ë¯¸ì§€ì™€ì˜ ìœ ì‚¬ë„
                guidance_scale=7.5,  # í”„ë¡¬í”„íŠ¸ ì¤€ìˆ˜ë„
                num_inference_steps=20,
                generator=generator
            )
            
            # ê²°ê³¼ ì €ì¥
            result.images[0].save(output_path)
            print(f"âœ… ì´ˆìƒí™” ìƒì„± ì™„ë£Œ: {output_path}")
            
            return output_path
            
        except Exception as e:
            print(f"âŒ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return None

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="ğŸ¨ ì…€ì¹´ë¥¼ ì´ˆìƒí™”ë¡œ ë³€í™˜í•©ë‹ˆë‹¤",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  python create_portrait.py --input my_photo.jpg
  python create_portrait.py --input my_photo.jpg --output my_portrait.png
  python create_portrait.py --input my_photo.jpg --emotion joyful
        """
    )
    parser.add_argument(
        "--input", "-i",
        type=str,
        required=True,
        help="ì…ë ¥ ì…€ì¹´ ì´ë¯¸ì§€ ê²½ë¡œ"
    )
    parser.add_argument(
        "--output", "-o",
        type=str,
        default=None,
        help="ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: portrait_[ì…ë ¥íŒŒì¼ëª…]_[ê°ì •].png)"
    )
    parser.add_argument(
        "--emotion", "-e",
        type=str,
        default=None,
        choices=["joyful", "neutral", "sad", "angry", "surprised", "contemplative"],
        help="ì›í•˜ëŠ” ê°ì • (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ìë™ ë¶„ì„)"
    )
    
    args = parser.parse_args()
    
    # ì…ë ¥ íŒŒì¼ í™•ì¸
    if not os.path.exists(args.input):
        print(f"âŒ ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.input}")
        return
    
    print("=" * 60)
    print("ğŸ¨ ì´ˆìƒí™” ìƒì„±ê¸° ì‹œì‘!")
    print("=" * 60)
    
    # ë³€í™˜ê¸° ì´ˆê¸°í™”
    converter = SelfieToPortrait()
    
    # í¬íŠ¸ë ˆì´íŠ¸ ìƒì„±
    result_path = converter.create_portrait(
        args.input,
        args.output,
        args.emotion
    )
    
    if result_path:
        print("\n" + "=" * 60)
        print(f"ğŸ‰ ì™„ë£Œ! ê²°ê³¼ ì´ë¯¸ì§€: {result_path}")
        print("=" * 60)
    else:
        print("\nâŒ ë³€í™˜ ì‹¤íŒ¨")

if __name__ == "__main__":
    main()


