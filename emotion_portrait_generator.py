#!/usr/bin/env python3
"""
ğŸ­ í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„ + LoRA ì´ˆìƒí™” ìƒì„± ì‹œìŠ¤í…œ
í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ë©´ ê°ì •ì„ ë¶„ì„í•˜ê³  í•´ë‹¹ ê°ì •ì— ë§ëŠ” ì´ˆìƒí™”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
"""

import torch
import argparse
import os
from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler
from transformers import pipeline
import re

class EmotionPortraitGenerator:
    def __init__(self, lora_model_path="./final"):
        """ê°ì • ì´ˆìƒí™” ìƒì„±ê¸° ì´ˆê¸°í™”"""
        print("ğŸš€ ê°ì • ì´ˆìƒí™” ìƒì„±ê¸° ì´ˆê¸°í™” ì¤‘...")
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ“± ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        # ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ (ë” ê°„ë‹¨í•œ ëª¨ë¸ ì‚¬ìš©)
        print("ğŸ§  ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì¤‘...")
        try:
            self.emotion_classifier = pipeline(
                "text-classification",
                model="cardiffnlp/twitter-roberta-base-emotion",
                device=0 if self.device == "cuda" else -1
            )
        except Exception as e:
            print(f"âš ï¸ ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ğŸ”„ ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì • ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            self.emotion_classifier = None
        
        # Stable Diffusion íŒŒì´í”„ë¼ì¸ ë¡œë“œ
        print("ğŸ¨ Stable Diffusion ëª¨ë¸ ë¡œë“œ ì¤‘...")
        self.pipe = StableDiffusionPipeline.from_pretrained(
            "runwayml/stable-diffusion-v1-5",
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            safety_checker=None,
            requires_safety_checker=False
        )
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        self.pipe.scheduler = DPMSolverMultistepScheduler.from_config(
            self.pipe.scheduler.config
        )
        
        # LoRA ëª¨ë¸ ë¡œë“œ
        if os.path.exists(lora_model_path):
            print(f"ğŸ­ LoRA ëª¨ë¸ ë¡œë“œ ì¤‘: {lora_model_path}")
            self.pipe.load_lora_weights(lora_model_path)
            print("âœ… LoRA ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        else:
            print(f"âš ï¸ LoRA ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {lora_model_path}")
        
        # GPUë¡œ ì´ë™
        if self.device == "cuda":
            self.pipe = self.pipe.to(self.device)
        
        print("âœ… ê°ì • ì´ˆìƒí™” ìƒì„±ê¸° ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def analyze_emotion(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ ê°ì • ë¶„ì„"""
        print(f"ğŸ” í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„ ì¤‘: '{text[:50]}...'")
        
        if self.emotion_classifier is not None:
            try:
                # ê°ì • ë¶„ì„ ì‹¤í–‰
                result = self.emotion_classifier(text)
                
                # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ê°ì • ì¶”ì¶œ
                primary_emotion = result[0]['label']
                confidence = result[0]['score']
                
                print(f"ğŸ’­ ê°ì • ë¶„ì„ ê²°ê³¼: {primary_emotion} (ì‹ ë¢°ë„: {confidence:.2f})")
                
                return primary_emotion, confidence
            except Exception as e:
                print(f"âš ï¸ ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
                return self._keyword_based_emotion(text)
        else:
            return self._keyword_based_emotion(text)
    
    def _keyword_based_emotion(self, text):
        """í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì • ë¶„ì„ (ëŒ€ì²´ ë°©ë²•)"""
        text_lower = text.lower()
        
        # ê°ì • í‚¤ì›Œë“œ ë§¤í•‘
        emotion_keywords = {
            'joy': ['happy', 'joy', 'excited', 'amazing', 'wonderful', 'great', 'fantastic', 'delighted', 'cheerful'],
            'love': ['love', 'adore', 'cherish', 'affection', 'romantic', 'caring', 'tender'],
            'anger': ['angry', 'mad', 'furious', 'rage', 'annoyed', 'irritated', 'frustrated'],
            'fear': ['afraid', 'scared', 'fear', 'worried', 'anxious', 'nervous', 'terrified'],
            'sadness': ['sad', 'depressed', 'melancholy', 'gloomy', 'unhappy', 'down', 'blue'],
            'surprise': ['surprised', 'shocked', 'amazed', 'astonished', 'wow', 'incredible'],
            'disgust': ['disgusted', 'revolted', 'sick', 'gross', 'nasty', 'repulsive']
        }
        
        # ê°ì • ì ìˆ˜ ê³„ì‚°
        emotion_scores = {}
        for emotion, keywords in emotion_keywords.items():
            score = sum(1 for keyword in keywords if keyword in text_lower)
            if score > 0:
                emotion_scores[emotion] = score
        
        if emotion_scores:
            # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ê°ì • ì„ íƒ
            primary_emotion = max(emotion_scores, key=emotion_scores.get)
            confidence = min(emotion_scores[primary_emotion] / len(text.split()), 1.0)
        else:
            primary_emotion = 'neutral'
            confidence = 0.5
        
        print(f"ğŸ’­ í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì • ë¶„ì„ ê²°ê³¼: {primary_emotion} (ì‹ ë¢°ë„: {confidence:.2f})")
        
        return primary_emotion, confidence
    
    def get_emotion_style(self, emotion):
        """ê°ì •ì— ë”°ë¥¸ ì´ˆìƒí™” ìŠ¤íƒ€ì¼ ë§¤í•‘"""
        emotion_styles = {
            'joy': "bright, cheerful, radiant smile, warm lighting, vibrant colors, optimistic expression",
            'love': "gentle, warm, tender expression, soft lighting, romantic atmosphere, caring eyes",
            'anger': "intense, powerful, dramatic lighting, strong facial features, bold expression, dynamic pose",
            'fear': "mysterious, shadowy, contemplative, subtle lighting, cautious expression, introspective",
            'sadness': "melancholic, thoughtful, gentle expression, soft lighting, contemplative mood, tender",
            'surprise': "animated, expressive, bright eyes, dynamic pose, energetic, lively expression",
            'disgust': "serious, composed, dignified expression, clean lighting, refined features, elegant",
            'neutral': "calm, peaceful, balanced expression, natural lighting, composed, serene"
        }
        
        return emotion_styles.get(emotion, emotion_styles['neutral'])
    
    def generate_portrait(self, text, output_path="generated_portrait.png", num_images=1):
        """í…ìŠ¤íŠ¸ì—ì„œ ê°ì •ì„ ë¶„ì„í•˜ê³  ì´ˆìƒí™” ìƒì„±"""
        print(f"\nğŸ­ ì´ˆìƒí™” ìƒì„± ì‹œì‘!")
        print(f"ğŸ“ ì…ë ¥ í…ìŠ¤íŠ¸: '{text}'")
        
        # 1. ê°ì • ë¶„ì„
        emotion, confidence = self.analyze_emotion(text)
        
        # 2. ê°ì •ì— ë”°ë¥¸ ìŠ¤íƒ€ì¼ ê²°ì •
        style = self.get_emotion_style(emotion)
        
        # 3. í”„ë¡¬í”„íŠ¸ ìƒì„± (ìµœì†Œí™”)
        full_prompt = f"a portrait, {emotion}"
        
        print(f"ğŸ¨ ìƒì„± í”„ë¡¬í”„íŠ¸: '{full_prompt}'")
        
        # 4. ì´ë¯¸ì§€ ìƒì„±
        print("ğŸ–¼ï¸ ì´ë¯¸ì§€ ìƒì„± ì¤‘...")
        images = self.pipe(
            full_prompt,
            num_inference_steps=20,
            num_images_per_prompt=num_images,
            guidance_scale=7.5
        ).images
        
        # 5. ì´ë¯¸ì§€ ì €ì¥
        if num_images == 1:
            images[0].save(output_path)
            print(f"âœ… ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: {output_path}")
        else:
            for i, image in enumerate(images):
                filename = f"generated_portrait_{i+1}.png"
                image.save(filename)
                print(f"âœ… ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: {filename}")
        
        return images, emotion, confidence
    
    def interactive_mode(self):
        """ëŒ€í™”í˜• ëª¨ë“œ"""
        print("\nğŸ­ ê°ì • ì´ˆìƒí™” ìƒì„±ê¸° ëŒ€í™”í˜• ëª¨ë“œ")
        print("ğŸ’¡ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ë©´ ê°ì •ì„ ë¶„ì„í•˜ê³  ì´ˆìƒí™”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        print("ğŸšª ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ë˜ëŠ” 'exit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        
        while True:
            try:
                text = input("\nğŸ“ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
                
                if text.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                    print("ğŸ‘‹ ê°ì • ì´ˆìƒí™” ìƒì„±ê¸°ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                if not text:
                    print("âš ï¸ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    continue
                
                # ì´ë¯¸ì§€ ìƒì„±
                images, emotion, confidence = self.generate_portrait(text)
                
                print(f"\nğŸ‰ ì™„ë£Œ! ê°ì •: {emotion} (ì‹ ë¢°ë„: {confidence:.2f})")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ê°ì • ì´ˆìƒí™” ìƒì„±ê¸°ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

def main():
    parser = argparse.ArgumentParser(description="í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„ + LoRA ì´ˆìƒí™” ìƒì„±")
    parser.add_argument("--text", type=str, help="ë¶„ì„í•  í…ìŠ¤íŠ¸")
    parser.add_argument("--output", type=str, default="generated_portrait.png", help="ì¶œë ¥ íŒŒì¼ëª…")
    parser.add_argument("--lora-path", type=str, default="./final", help="LoRA ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--interactive", action="store_true", help="ëŒ€í™”í˜• ëª¨ë“œ")
    parser.add_argument("--num-images", type=int, default=1, help="ìƒì„±í•  ì´ë¯¸ì§€ ê°œìˆ˜")
    
    args = parser.parse_args()
    
    # ìƒì„±ê¸° ì´ˆê¸°í™”
    generator = EmotionPortraitGenerator(lora_model_path=args.lora_path)
    
    if args.interactive:
        # ëŒ€í™”í˜• ëª¨ë“œ
        generator.interactive_mode()
    elif args.text:
        # ë‹¨ì¼ í…ìŠ¤íŠ¸ ì²˜ë¦¬
        images, emotion, confidence = generator.generate_portrait(
            args.text, 
            args.output, 
            args.num_images
        )
        print(f"\nğŸ‰ ì™„ë£Œ! ê°ì •: {emotion} (ì‹ ë¢°ë„: {confidence:.2f})")
    else:
        print("âŒ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ --interactive ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        print("ğŸ’¡ ì˜ˆì‹œ: python emotion_portrait_generator.py --text 'I am so happy today!'")

if __name__ == "__main__":
    main()
