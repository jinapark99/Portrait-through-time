#!/usr/bin/env python3
"""
ğŸš€ ë¹ ë¥¸ ì…€ì¹´â†’ì´ˆìƒí™” ë³€í™˜ê¸° (ê°„ë‹¨ ë²„ì „)
DeepFace ì—†ì´ MediaPipeë§Œ ì‚¬ìš©í•´ì„œ ë¹ ë¥´ê²Œ ì‘ë™í•©ë‹ˆë‹¤.
"""

import torch
import argparse
import os
import cv2
import numpy as np
from PIL import Image
from diffusers import StableDiffusionImg2ImgPipeline
import mediapipe as mp

class QuickSelfieToPortrait:
    def __init__(self, lora_model_path="lora_trained_model/final"):
        """ë¹ ë¥¸ ì…€ì¹´â†’ì´ˆìƒí™” ë³€í™˜ê¸° ì´ˆê¸°í™”"""
        print("ğŸš€ ë¹ ë¥¸ ì…€ì¹´â†’ì´ˆìƒí™” ë³€í™˜ê¸° ì´ˆê¸°í™” ì¤‘...")
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ“± ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        # MediaPipe Face Detection ì´ˆê¸°í™”
        print("ğŸ‘¤ ì–¼êµ´ ê²€ì¶œ ëª¨ë¸ ë¡œë“œ ì¤‘...")
        self.mp_face_detection = mp.solutions.face_detection
        self.face_detection = self.mp_face_detection.FaceDetection(
            model_selection=1, min_detection_confidence=0.5
        )
        
        print("âœ… ë¹ ë¥¸ ë³€í™˜ê¸° ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def detect_face(self, image_path):
        """ì´ë¯¸ì§€ì—ì„œ ì–¼êµ´ ê²€ì¶œ"""
        print(f"ğŸ‘¤ ì–¼êµ´ ê²€ì¶œ ì¤‘: {image_path}")
        
        # ì´ë¯¸ì§€ ì½ê¸°
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"ì´ë¯¸ì§€ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
        
        # RGBë¡œ ë³€í™˜
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # ì–¼êµ´ ê²€ì¶œ
        results = self.face_detection.process(image_rgb)
        
        if not results.detections:
            raise ValueError("ì–¼êµ´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        
        print(f"âœ… {len(results.detections)}ê°œì˜ ì–¼êµ´ ë°œê²¬!")
        return image, image_rgb, results.detections
    
    def simple_emotion_analysis(self, image_path):
        """ê°„ë‹¨í•œ ê°ì • ë¶„ì„ (ê¸°ë³¸ê°’)"""
        print("ğŸ’­ ê°„ë‹¨í•œ ê°ì • ë¶„ì„: ê¸°ë³¸ê°’ 'joy' ì‚¬ìš©")
        return "joy", {"joy": 0.8}
    
    def get_emotion_prompt(self, emotion):
        """ê°ì •ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        emotion_prompts = {
            'joy': "a joyful portrait, happy expression, warm smile, bright eyes, cheerful, beautiful lighting",
            'sadness': "a melancholic portrait, sad expression, contemplative gaze, gentle mood, soft lighting",
            'anger': "a powerful portrait, intense expression, strong features, dramatic lighting",
            'fear': "a thoughtful portrait, cautious expression, mysterious atmosphere, subtle lighting",
            'surprise': "an expressive portrait, surprised expression, wide eyes, dynamic pose",
            'disgust': "a dignified portrait, composed expression, refined features, elegant lighting",
            'neutral': "a serene portrait, calm expression, peaceful atmosphere, balanced lighting",
            'love': "a tender portrait, gentle expression, warm atmosphere, caring eyes, romantic lighting"
        }
        return emotion_prompts.get(emotion, emotion_prompts['joy'])
    
    def create_portrait_fast(self, image_path, output_path="my_portrait_fast.png", emotion="joy"):
        """
        ë¹ ë¥¸ ì´ˆìƒí™” ìƒì„± (Img2Img ì‚¬ìš©)
        """
        print(f"\nğŸ¨ ë¹ ë¥¸ ì´ˆìƒí™” ìƒì„± ì‹œì‘!")
        print(f"ğŸ“¸ ì…ë ¥ ì´ë¯¸ì§€: {image_path}")
        print(f"ğŸ­ ì‚¬ìš©í•  ê°ì •: {emotion}")
        
        # 1. ì–¼êµ´ ê²€ì¶œ
        try:
            image, image_rgb, detections = self.detect_face(image_path)
        except Exception as e:
            print(f"âš ï¸ ì–¼êµ´ ê²€ì¶œ ì‹¤íŒ¨: {e}")
            print("ğŸ’¡ ì–¼êµ´ì´ ì—†ëŠ” ì´ë¯¸ì§€ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
        
        # 2. í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self.get_emotion_prompt(emotion)
        print(f"ğŸ“ ìƒì„± í”„ë¡¬í”„íŠ¸: {prompt}")
        
        # 3. Img2Img íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ìƒì„±
        print("ğŸ¨ Img2Imgë¡œ ì´ë¯¸ì§€ ìƒì„± ì¤‘...")
        
        # íŒŒì´í”„ë¼ì¸ ë¡œë“œ
        pipe = StableDiffusionImg2ImgPipeline.from_pretrained(
            "runwayml/stable-diffusion-v1-5",
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            safety_checker=None,
            requires_safety_checker=False
        )
        
        # LoRA ëª¨ë¸ ë¡œë“œ
        lora_path = "lora_trained_model/final"
        if os.path.exists(lora_path):
            print(f"ğŸ­ LoRA ëª¨ë¸ ë¡œë“œ: {lora_path}")
            pipe.load_lora_weights(lora_path)
        
        if self.device == "cuda":
            pipe = pipe.to(self.device)
        
        # ì›ë³¸ ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬
        init_image = Image.open(image_path).convert("RGB")
        init_image = init_image.resize((512, 512))
        
        # ì´ë¯¸ì§€ ìƒì„± (strength: 0.7 = ì›ë³¸ì˜ 30% ìœ ì§€, 70% ìƒˆë¡œ ìƒì„±)
        print("ğŸ–¼ï¸ ì´ë¯¸ì§€ ìƒì„± ì¤‘... (ì•½ 2-3ë¶„ ì†Œìš”)")
        result = pipe(
            prompt=prompt,
            image=init_image,
            strength=0.7,  # ì›ë³¸ ì–¼êµ´ êµ¬ì¡°ë¥¼ ë” ì˜ ìœ ì§€
            guidance_scale=7.5,
            num_inference_steps=20  # ë” ë¹ ë¥´ê²Œ (ê¸°ì¡´ 30ì—ì„œ 20ìœ¼ë¡œ)
        )
        
        # ì €ì¥
        result.images[0].save(output_path)
        print(f"âœ… ì´ˆìƒí™” ì €ì¥ ì™„ë£Œ: {output_path}")
        
        return result.images[0], emotion


def main():
    parser = argparse.ArgumentParser(description="ë¹ ë¥¸ ì…€ì¹´â†’ì´ˆìƒí™” ë³€í™˜")
    parser.add_argument("--input", type=str, required=True, help="ì…ë ¥ ì…€ì¹´ ì´ë¯¸ì§€ ê²½ë¡œ")
    parser.add_argument("--output", type=str, default="my_portrait_fast.png", help="ì¶œë ¥ íŒŒì¼ëª…")
    parser.add_argument("--emotion", type=str, default="joy", 
                       choices=["joy", "sadness", "anger", "fear", "surprise", "neutral", "love"],
                       help="ì›í•˜ëŠ” ê°ì •")
    parser.add_argument("--lora-path", type=str, default="lora_trained_model/final", 
                       help="LoRA ëª¨ë¸ ê²½ë¡œ")
    
    args = parser.parse_args()
    
    # ì…ë ¥ íŒŒì¼ í™•ì¸
    if not os.path.exists(args.input):
        print(f"âŒ ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.input}")
        return
    
    # ë³€í™˜ê¸° ì´ˆê¸°í™”
    converter = QuickSelfieToPortrait(lora_model_path=args.lora_path)
    
    # ì´ˆìƒí™” ìƒì„±
    image, emotion = converter.create_portrait_fast(
        args.input, 
        args.output,
        args.emotion
    )
    
    print(f"\nğŸ‰ ì™„ë£Œ!")
    print(f"ğŸ­ ê°ì •: {emotion}")
    print(f"ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {args.output}")

if __name__ == "__main__":
    main()





