#!/usr/bin/env python3
"""
ğŸ“¸ ì…€ì¹´ â†’ ê°ì • ë¶„ì„ â†’ ì´ˆìƒí™” ìƒì„± ì‹œìŠ¤í…œ
ì…€ì¹´ë¥¼ ì…ë ¥í•˜ë©´ ì–¼êµ´ì˜ ê°ì •ì„ ë¶„ì„í•˜ê³ , ê·¸ ê°ì •ê³¼ ì–¼êµ´ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë©´ì„œ
í•™ìŠµëœ ì´ˆìƒí™” ìŠ¤íƒ€ì¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
"""

import torch
import argparse
import os
import cv2
import numpy as np
from PIL import Image
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
from diffusers.utils import load_image
from transformers import pipeline
import mediapipe as mp

# ê°„ë‹¨í•œ ê°ì • ë¶„ì„ì„ ìœ„í•œ DeepFace ëŒ€ì²´
try:
    from deepface import DeepFace
    DEEPFACE_AVAILABLE = True
except ImportError:
    DEEPFACE_AVAILABLE = False
    print("âš ï¸ DeepFaceë¥¼ ì„¤ì¹˜í•˜ë©´ ë” ì •í™•í•œ ê°ì • ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤: pip install deepface")

class SelfieToPortrait:
    def __init__(self, lora_model_path="lora_trained_model/final"):
        """ì…€ì¹´ë¥¼ ì´ˆìƒí™”ë¡œ ë³€í™˜í•˜ëŠ” ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        print("ğŸš€ ì…€ì¹´â†’ì´ˆìƒí™” ë³€í™˜ê¸° ì´ˆê¸°í™” ì¤‘...")
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ“± ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        # MediaPipe Face Detection ì´ˆê¸°í™” (ì–¼êµ´ ê²€ì¶œìš©)
        print("ğŸ‘¤ ì–¼êµ´ ê²€ì¶œ ëª¨ë¸ ë¡œë“œ ì¤‘...")
        self.mp_face_detection = mp.solutions.face_detection
        self.mp_drawing = mp.solutions.drawing_utils
        self.face_detection = self.mp_face_detection.FaceDetection(
            model_selection=1, min_detection_confidence=0.5
        )
        
        print("âœ… ì…€ì¹´â†’ì´ˆìƒí™” ë³€í™˜ê¸° ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def detect_face(self, image_path):
        """ì´ë¯¸ì§€ì—ì„œ ì–¼êµ´ ê²€ì¶œ"""
        print(f"ğŸ‘¤ ì–¼êµ´ ê²€ì¶œ ì¤‘: {image_path}")
        
        # ì´ë¯¸ì§€ ì½ê¸°
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"ì´ë¯¸ì§€ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
        
        # RGBë¡œ ë³€í™˜
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # ì–¼êµ´ ê²€ì¶œ
        results = self.face_detection.process(image_rgb)
        
        if not results.detections:
            raise ValueError("ì–¼êµ´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        
        print(f"âœ… {len(results.detections)}ê°œì˜ ì–¼êµ´ ë°œê²¬!")
        return image, image_rgb, results.detections
    
    def analyze_emotion_from_face(self, image_path):
        """ì–¼êµ´ì—ì„œ ê°ì • ë¶„ì„"""
        print("ğŸ” ì–¼êµ´ ê°ì • ë¶„ì„ ì¤‘...")
        
        if DEEPFACE_AVAILABLE:
            try:
                # DeepFaceë¡œ ê°ì • ë¶„ì„
                analysis = DeepFace.analyze(
                    img_path=image_path, 
                    actions=['emotion'],
                    enforce_detection=False
                )
                
                # ê²°ê³¼ ì²˜ë¦¬
                if isinstance(analysis, list):
                    analysis = analysis[0]
                
                emotions = analysis['emotion']
                dominant_emotion = analysis['dominant_emotion']
                
                print(f"ğŸ’­ ê°ì§€ëœ ê°ì •: {dominant_emotion}")
                print(f"ğŸ“Š ê°ì • ì ìˆ˜: {emotions}")
                
                return self._map_emotion(dominant_emotion), emotions
            
            except Exception as e:
                print(f"âš ï¸ DeepFace ê°ì • ë¶„ì„ ì‹¤íŒ¨: {e}")
                return self._simple_emotion_analysis(image_path)
        else:
            return self._simple_emotion_analysis(image_path)
    
    def _simple_emotion_analysis(self, image_path):
        """ê°„ë‹¨í•œ ê°ì • ë¶„ì„ (ëŒ€ì²´ ë°©ë²•)"""
        print("ğŸ’­ ê°„ë‹¨í•œ ê°ì • ë¶„ì„ ì‚¬ìš© ì¤‘...")
        # ê¸°ë³¸ê°’ìœ¼ë¡œ neutral ë°˜í™˜
        return "neutral", {"neutral": 0.8}
    
    def _map_emotion(self, deepface_emotion):
        """DeepFace ê°ì •ì„ ìš°ë¦¬ ì‹œìŠ¤í…œì˜ ê°ì •ìœ¼ë¡œ ë§¤í•‘"""
        emotion_map = {
            'happy': 'joy',
            'sad': 'sadness',
            'angry': 'anger',
            'surprise': 'surprise',
            'fear': 'fear',
            'disgust': 'disgust',
            'neutral': 'neutral'
        }
        return emotion_map.get(deepface_emotion.lower(), 'neutral')
    
    def get_emotion_prompt(self, emotion):
        """ê°ì •ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        emotion_prompts = {
            'joy': "a joyful portrait, happy expression, warm smile, bright eyes, cheerful",
            'sadness': "a melancholic portrait, sad expression, contemplative gaze, gentle mood",
            'anger': "a powerful portrait, intense expression, strong features, dramatic",
            'fear': "a thoughtful portrait, cautious expression, mysterious atmosphere",
            'surprise': "an expressive portrait, surprised expression, wide eyes, dynamic",
            'disgust': "a dignified portrait, composed expression, refined features",
            'neutral': "a serene portrait, calm expression, peaceful atmosphere, balanced",
            'love': "a tender portrait, gentle expression, warm atmosphere, caring eyes"
        }
        return emotion_prompts.get(emotion, emotion_prompts['neutral'])
    
    def create_portrait_simple(self, image_path, output_path="selfie_portrait.png"):
        """
        ê°„ë‹¨í•œ ë°©ë²•: ì…€ì¹´ + ê°ì • â†’ ì´ˆìƒí™” ìƒì„±
        (ControlNet ì—†ì´, í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë§Œ ì‚¬ìš©)
        """
        print(f"\nğŸ¨ ì´ˆìƒí™” ìƒì„± ì‹œì‘!")
        print(f"ğŸ“¸ ì…ë ¥ ì´ë¯¸ì§€: {image_path}")
        
        # 1. ì–¼êµ´ ê²€ì¶œ
        image, image_rgb, detections = self.detect_face(image_path)
        
        # 2. ê°ì • ë¶„ì„
        emotion, emotion_scores = self.analyze_emotion_from_face(image_path)
        print(f"ğŸ­ ë¶„ì„ëœ ê°ì •: {emotion}")
        
        # 3. í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self.get_emotion_prompt(emotion)
        print(f"ğŸ“ ìƒì„± í”„ë¡¬í”„íŠ¸: {prompt}")
        
        # 4. Stable Diffusionìœ¼ë¡œ ì´ë¯¸ì§€ ìƒì„±
        print("ğŸ¨ ì´ë¯¸ì§€ ìƒì„± ì¤‘... (LoRA ëª¨ë¸ ì‚¬ìš©)")
        print("ğŸ’¡ íŒ: ControlNetì„ ì‚¬ìš©í•˜ë©´ ì–¼êµ´ êµ¬ì¡°ë¥¼ ë” ì˜ ìœ ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
        
        from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler
        
        # íŒŒì´í”„ë¼ì¸ ë¡œë“œ
        pipe = StableDiffusionPipeline.from_pretrained(
            "runwayml/stable-diffusion-v1-5",
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            safety_checker=None,
            requires_safety_checker=False
        )
        
        pipe.scheduler = DPMSolverMultistepScheduler.from_config(
            pipe.scheduler.config
        )
        
        # LoRA ëª¨ë¸ ë¡œë“œ
        lora_path = "lora_trained_model/final"
        if os.path.exists(lora_path):
            print(f"ğŸ­ LoRA ëª¨ë¸ ë¡œë“œ: {lora_path}")
            pipe.load_lora_weights(lora_path)
        
        if self.device == "cuda":
            pipe = pipe.to(self.device)
        
        # ì´ë¯¸ì§€ ìƒì„±
        result = pipe(
            prompt,
            num_inference_steps=30,
            guidance_scale=7.5
        )
        
        # ì €ì¥
        result.images[0].save(output_path)
        print(f"âœ… ì´ˆìƒí™” ì €ì¥ ì™„ë£Œ: {output_path}")
        
        return result.images[0], emotion, emotion_scores
    
    def create_portrait_with_img2img(self, image_path, output_path="selfie_portrait.png"):
        """
        Img2Img ë°©ë²•: ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ì°¸ì¡°í•˜ì—¬ ì´ˆìƒí™” ìƒì„±
        ì–¼êµ´ êµ¬ì¡°ë¥¼ ë” ì˜ ìœ ì§€í•©ë‹ˆë‹¤!
        """
        print(f"\nğŸ¨ Img2Img ì´ˆìƒí™” ìƒì„± ì‹œì‘!")
        print(f"ğŸ“¸ ì…ë ¥ ì´ë¯¸ì§€: {image_path}")
        
        # 1. ì–¼êµ´ ê²€ì¶œ
        image, image_rgb, detections = self.detect_face(image_path)
        
        # 2. ê°ì • ë¶„ì„
        emotion, emotion_scores = self.analyze_emotion_from_face(image_path)
        print(f"ğŸ­ ë¶„ì„ëœ ê°ì •: {emotion}")
        
        # 3. í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self.get_emotion_prompt(emotion)
        print(f"ğŸ“ ìƒì„± í”„ë¡¬í”„íŠ¸: {prompt}")
        
        # 4. Img2Img íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ìƒì„±
        print("ğŸ¨ Img2Imgë¡œ ì´ë¯¸ì§€ ìƒì„± ì¤‘...")
        
        from diffusers import StableDiffusionImg2ImgPipeline
        
        # íŒŒì´í”„ë¼ì¸ ë¡œë“œ
        pipe = StableDiffusionImg2ImgPipeline.from_pretrained(
            "runwayml/stable-diffusion-v1-5",
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            safety_checker=None,
            requires_safety_checker=False
        )
        
        # LoRA ëª¨ë¸ ë¡œë“œ
        lora_path = "lora_trained_model/final"
        if os.path.exists(lora_path):
            print(f"ğŸ­ LoRA ëª¨ë¸ ë¡œë“œ: {lora_path}")
            pipe.load_lora_weights(lora_path)
        
        if self.device == "cuda":
            pipe = pipe.to(self.device)
        
        # ì›ë³¸ ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬
        init_image = Image.open(image_path).convert("RGB")
        init_image = init_image.resize((512, 512))
        
        # ì´ë¯¸ì§€ ìƒì„± (strength: 0.75 = ì›ë³¸ì˜ 25% ìœ ì§€, 75% ìƒˆë¡œ ìƒì„±)
        result = pipe(
            prompt=prompt,
            image=init_image,
            strength=0.75,  # 0.0 = ì›ë³¸ ê·¸ëŒ€ë¡œ, 1.0 = ì™„ì „íˆ ìƒˆë¡œ ìƒì„±
            guidance_scale=7.5,
            num_inference_steps=30
        )
        
        # ì €ì¥
        result.images[0].save(output_path)
        print(f"âœ… ì´ˆìƒí™” ì €ì¥ ì™„ë£Œ: {output_path}")
        
        return result.images[0], emotion, emotion_scores


def main():
    parser = argparse.ArgumentParser(description="ì…€ì¹´ë¥¼ ì´ˆìƒí™”ë¡œ ë³€í™˜")
    parser.add_argument("--input", type=str, required=True, help="ì…ë ¥ ì…€ì¹´ ì´ë¯¸ì§€ ê²½ë¡œ")
    parser.add_argument("--output", type=str, default="selfie_portrait.png", help="ì¶œë ¥ íŒŒì¼ëª…")
    parser.add_argument("--method", type=str, default="img2img", 
                       choices=["simple", "img2img"],
                       help="ìƒì„± ë°©ë²• (simple: í…ìŠ¤íŠ¸ë§Œ, img2img: ì›ë³¸ ì°¸ì¡°)")
    parser.add_argument("--lora-path", type=str, default="lora_trained_model/final", 
                       help="LoRA ëª¨ë¸ ê²½ë¡œ")
    
    args = parser.parse_args()
    
    # ì…ë ¥ íŒŒì¼ í™•ì¸
    if not os.path.exists(args.input):
        print(f"âŒ ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.input}")
        return
    
    # ë³€í™˜ê¸° ì´ˆê¸°í™”
    converter = SelfieToPortrait(lora_model_path=args.lora_path)
    
    # ì´ˆìƒí™” ìƒì„±
    if args.method == "img2img":
        image, emotion, scores = converter.create_portrait_with_img2img(
            args.input, 
            args.output
        )
    else:
        image, emotion, scores = converter.create_portrait_simple(
            args.input, 
            args.output
        )
    
    print(f"\nğŸ‰ ì™„ë£Œ!")
    print(f"ğŸ­ ê°ì •: {emotion}")
    print(f"ğŸ“Š ê°ì • ì ìˆ˜: {scores}")
    print(f"ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {args.output}")

if __name__ == "__main__":
    main()






