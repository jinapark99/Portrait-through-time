#!/usr/bin/env python3
"""
ğŸ¨ ê°ì •ë³„ íšŒí™” ìŠ¤íƒ€ì¼ í¬íŠ¸ë ˆì´íŠ¸ ìƒì„±ê¸°
ê° ê°ì •ì— ë§ëŠ” íŠ¹ì • íšŒí™” ìŠ¤íƒ€ì¼ì„ ì ìš©í•©ë‹ˆë‹¤.
"""

import torch
import cv2
import numpy as np
from PIL import Image
from diffusers import StableDiffusionImg2ImgPipeline
import mediapipe as mp

class EmotionStylePortrait:
    def __init__(self):
        print("ğŸ¨ ê°ì •ë³„ íšŒí™” ìŠ¤íƒ€ì¼ í¬íŠ¸ë ˆì´íŠ¸ ìƒì„±ê¸° ì´ˆê¸°í™” ì¤‘...")
        
        # M1 GPU ì„¤ì •
        self.device = "mps" if torch.backends.mps.is_available() else "cpu"
        print(f"ğŸ“± ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        # MediaPipe ì–¼êµ´ ê°ì§€ ì´ˆê¸°í™”
        self.mp_face_detection = mp.solutions.face_detection
        self.face_detection = self.mp_face_detection.FaceDetection(
            model_selection=0, min_detection_confidence=0.5
        )
        
        # Stable Diffusion íŒŒì´í”„ë¼ì¸ ë¡œë“œ
        print("ğŸ¨ Stable Diffusion íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì¤‘...")
        self.pipe = StableDiffusionImg2ImgPipeline.from_pretrained(
            "runwayml/stable-diffusion-v1-5",
            torch_dtype=torch.float16 if self.device in ["mps", "cuda"] else torch.float32,
            safety_checker=None,
            requires_safety_checker=False
        )
        self.pipe = self.pipe.to(self.device)
        
        # LoRA ë¡œë“œ
        print("ğŸ­ LoRA ëª¨ë¸ ë¡œë“œ ì¤‘...")
        try:
            self.pipe.load_lora_weights("lora_trained_model/final")
            print("âœ… LoRA ë¡œë“œ ì„±ê³µ!")
        except Exception as e:
            print(f"âŒ LoRA ë¡œë“œ ì‹¤íŒ¨: {e}")
            return
        
        # ê°ì •ë³„ íšŒí™” ìŠ¤íƒ€ì¼ ì •ì˜
        self.emotion_styles = {
            "joyful": {
                "style": "renaissance portrait, bright and warm colors, golden lighting, cheerful expression, baroque style, vibrant colors, optimistic mood",
                "description": "ë¥´ë„¤ìƒìŠ¤/ë°”ë¡œí¬ ìŠ¤íƒ€ì¼ - ë°ê³  ë”°ëœ»í•œ ìƒ‰ì¡°"
            },
            "sad": {
                "style": "medieval portrait, dark and muted colors, melancholic expression, gothic style, somber mood, chiaroscuro lighting, contemplative",
                "description": "ì¤‘ì„¸/ê³ ë”• ìŠ¤íƒ€ì¼ - ì–´ë‘¡ê³  ì°¨ë¶„í•œ ìƒ‰ì¡°"
            },
            "angry": {
                "style": "baroque portrait, dramatic lighting, intense expression, bold colors, dynamic composition, powerful mood, dramatic shadows",
                "description": "ë°”ë¡œí¬ ìŠ¤íƒ€ì¼ - ë“œë¼ë§ˆí‹±í•˜ê³  ê°•ë ¬í•œ ìƒ‰ì¡°"
            },
            "surprised": {
                "style": "rococo portrait, elegant and refined, soft pastel colors, delicate expression, ornate details, graceful mood, refined style",
                "description": "ë¡œì½”ì½” ìŠ¤íƒ€ì¼ - ìš°ì•„í•˜ê³  ì„¬ì„¸í•œ ìƒ‰ì¡°"
            },
            "neutral": {
                "style": "classical portrait, balanced composition, natural colors, serene expression, timeless style, harmonious mood, classical proportions",
                "description": "í´ë˜ì‹ ìŠ¤íƒ€ì¼ - ê· í˜•ì¡íŒ ìì—°ìŠ¤ëŸ¬ìš´ ìƒ‰ì¡°"
            },
            "contemplative": {
                "style": "romantic portrait, soft and dreamy colors, thoughtful expression, ethereal mood, soft lighting, introspective, romantic era style",
                "description": "ë‚­ë§Œì£¼ì˜ ìŠ¤íƒ€ì¼ - ë¶€ë“œëŸ½ê³  ëª½í™˜ì ì¸ ìƒ‰ì¡°"
            }
        }
        
        print("âœ… ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def detect_face(self, image):
        """ì–¼êµ´ ê°ì§€"""
        cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        results = self.face_detection.process(cv_image)
        
        if results.detections:
            detection = results.detections[0]
            bbox = detection.location_data.relative_bounding_box
            
            h, w, _ = cv_image.shape
            x = int(bbox.xmin * w)
            y = int(bbox.ymin * h)
            width = int(bbox.width * w)
            height = int(bbox.height * h)
            
            face_crop = image.crop((x, y, x + width, y + height))
            print(f"âœ… ì–¼êµ´ ê°ì§€ ì„±ê³µ: {width}x{height}")
            return face_crop, True
        else:
            print("âŒ ì–¼êµ´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return image, False
    
    def analyze_emotion_advanced(self, face_image):
        """ê³ ê¸‰ ê°ì • ë¶„ì„"""
        # ì–¼êµ´ ëœë“œë§ˆí¬ ê°ì§€
        mp_face_mesh = mp.solutions.face_mesh
        face_mesh = mp_face_mesh.FaceMesh(
            static_image_mode=True,
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.3,
            min_tracking_confidence=0.3
        )
        
        img_array = np.array(face_image)
        results = face_mesh.process(img_array)
        
        if results.multi_face_landmarks:
            landmarks = results.multi_face_landmarks[0]
            
            # ë‹¤ì–‘í•œ ì–¼êµ´ íŠ¹ì§• ë¶„ì„
            # ì…ê¼¬ë¦¬ (61, 291)
            left_corner = landmarks.landmark[61]
            right_corner = landmarks.landmark[291]
            mouth_curve = right_corner.y - left_corner.y
            
            # ëˆˆì¹ ë†’ì´ (70, 300)
            left_eyebrow = landmarks.landmark[70]
            right_eyebrow = landmarks.landmark[300]
            eyebrow_height = (left_eyebrow.y + right_eyebrow.y) / 2
            
            # ëˆˆ í¬ê¸° (33, 7)
            left_eye = landmarks.landmark[33]
            right_eye = landmarks.landmark[7]
            eye_openness = abs(left_eye.y - right_eye.y)
            
            print(f"ğŸ” ì–¼êµ´ ë¶„ì„:")
            print(f"   ì…ê¼¬ë¦¬ ê³¡ì„ : {mouth_curve:.4f}")
            print(f"   ëˆˆì¹ ë†’ì´: {eyebrow_height:.4f}")
            print(f"   ëˆˆ í¬ê¸°: {eye_openness:.4f}")
            
            # ê°ì • íŒë‹¨ ë¡œì§ (ì„ê³„ê°’ ì¡°ì •)
            if mouth_curve < -0.005 and eyebrow_height < 0.4:
                emotion = "joyful"
                print("ğŸ˜Š ê°ì •: ê¸°ì¨ (ì›ƒëŠ” í‘œì • + ë‚®ì€ ëˆˆì¹)")
            elif mouth_curve > 0.01:  # ì„ê³„ê°’ ë‚®ì¶¤ (0.005 â†’ 0.01)
                emotion = "sad"
                print("ğŸ˜¢ ê°ì •: ìŠ¬í”” (ë‚´ë¦° ì…ê¼¬ë¦¬)")
            elif eye_openness > 0.02 and eyebrow_height < 0.4:
                emotion = "surprised"
                print("ğŸ˜² ê°ì •: ë†€ëŒ (í¬ê²Œ ëœ¬ ëˆˆ + ë‚®ì€ ëˆˆì¹)")
            elif eyebrow_height > 0.45 and mouth_curve < -0.002:
                emotion = "angry"
                print("ğŸ˜  ê°ì •: í™”ë‚¨ (ë†’ì€ ëˆˆì¹ + ì•½ê°„ ì›ƒëŠ” ì…)")
            elif eyebrow_height > 0.42:
                emotion = "contemplative"
                print("ğŸ¤” ê°ì •: ì‚¬ìƒ‰ (ë†’ì€ ëˆˆì¹)")
            else:
                emotion = "neutral"
                print("ğŸ˜ ê°ì •: ì¤‘ë¦½")
        else:
            # ëœë“œë§ˆí¬ ê°ì§€ ì‹¤íŒ¨ ì‹œ ë” ë³´ìˆ˜ì ìœ¼ë¡œ íŒë‹¨
            img_array = np.array(face_image)
            brightness = np.mean(img_array)
            contrast = np.std(img_array)
            
            print(f"ğŸ” ì´ë¯¸ì§€ ë¶„ì„:")
            print(f"   ë°ê¸°: {brightness:.2f}")
            print(f"   ëŒ€ë¹„: {contrast:.2f}")
            
            # ë” ë³´ìˆ˜ì ì¸ ê°ì • íŒë‹¨ (ëœë“œë§ˆí¬ ì—†ì´ëŠ” ì¤‘ë¦½ìœ¼ë¡œ)
            if brightness < 80:  # ë§¤ìš° ì–´ë‘ìš´ ê²½ìš°ë§Œ ìŠ¬í””
                emotion = "sad"
                print("ğŸ˜¢ ê°ì •: ìŠ¬í”” (ë§¤ìš° ì–´ë‘ìš´ ì´ë¯¸ì§€)")
            elif brightness > 150 and contrast > 60:  # ë§¤ìš° ë°ê³  ëŒ€ë¹„ê°€ ë†’ì€ ê²½ìš°ë§Œ ê¸°ì¨
                emotion = "joyful"
                print("ğŸ˜Š ê°ì •: ê¸°ì¨ (ë§¤ìš° ë°ê³  ëŒ€ë¹„ ë†’ì€ ì´ë¯¸ì§€)")
            else:
                emotion = "neutral"  # ê¸°ë³¸ê°’ì„ ì¤‘ë¦½ìœ¼ë¡œ
                print("ğŸ˜ ê°ì •: ì¤‘ë¦½ (ëœë“œë§ˆí¬ ê°ì§€ ì‹¤íŒ¨, ë³´ìˆ˜ì  íŒë‹¨)")
        
        return emotion
    
    def get_emotion_style_prompt(self, emotion):
        """ê°ì •ì— ë§ëŠ” íšŒí™” ìŠ¤íƒ€ì¼ í”„ë¡¬í”„íŠ¸ ë°˜í™˜"""
        if emotion in self.emotion_styles:
            style_info = self.emotion_styles[emotion]
            print(f"ğŸ¨ ì ìš© ìŠ¤íƒ€ì¼: {style_info['description']}")
            return style_info["style"]
        else:
            print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ê°ì •: {emotion}, ê¸°ë³¸ ìŠ¤íƒ€ì¼ ì‚¬ìš©")
            return self.emotion_styles["neutral"]["style"]
    
    def create_emotion_portrait(self, selfie_path):
        """ê°ì •ë³„ íšŒí™” ìŠ¤íƒ€ì¼ë¡œ í¬íŠ¸ë ˆì´íŠ¸ ìƒì„±"""
        print(f"\nğŸ¨ ê°ì •ë³„ íšŒí™” ìŠ¤íƒ€ì¼ í¬íŠ¸ë ˆì´íŠ¸ ìƒì„± ì‹œì‘: {selfie_path}")
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        try:
            image = Image.open(selfie_path).convert("RGB")
            print(f"âœ… ì´ë¯¸ì§€ ë¡œë“œ ì„±ê³µ: {image.size}")
        except Exception as e:
            print(f"âŒ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
        
        # ì–¼êµ´ ê°ì§€
        face_crop, face_found = self.detect_face(image)
        input_image = face_crop.resize((512, 512))
        
        # ê³ ê¸‰ ê°ì • ë¶„ì„
        emotion = self.analyze_emotion_advanced(input_image)
        
        # ê°ì •ë³„ íšŒí™” ìŠ¤íƒ€ì¼ í”„ë¡¬í”„íŠ¸ ìƒì„±
        style_prompt = self.get_emotion_style_prompt(emotion)
        
        # ìµœì¢… í”„ë¡¬í”„íŠ¸ ìƒì„±
        final_prompt = f"portrait painting, {style_prompt}"
        print(f"ğŸ“ ìµœì¢… í”„ë¡¬í”„íŠ¸: {final_prompt}")
        
        # ì´ë¯¸ì§€ ìƒì„±
        print("ğŸ¨ ê°ì •ë³„ íšŒí™” ìŠ¤íƒ€ì¼ ì´ë¯¸ì§€ ìƒì„± ì¤‘... (ì•½ 2-3ë¶„ ì†Œìš”)")
        try:
            result = self.pipe(
                prompt=final_prompt,
                image=input_image,
                strength=0.6,  # ìŠ¤íƒ€ì¼ ì ìš©ì„ ìœ„í•´ ì¡°ì •
                guidance_scale=8.0,  # ìŠ¤íƒ€ì¼ ì¤€ìˆ˜ë„ ë†’ì„
                num_inference_steps=25,  # í’ˆì§ˆ í–¥ìƒ
                generator=torch.Generator(device=self.device)
            )
            
            # ê²°ê³¼ ì €ì¥
            output_path = f"emotion_style_portrait_{emotion}.png"
            result.images[0].save(output_path)
            print(f"âœ… ê°ì •ë³„ íšŒí™” ìŠ¤íƒ€ì¼ í¬íŠ¸ë ˆì´íŠ¸ ìƒì„± ì™„ë£Œ: {output_path}")
            
            return output_path, emotion
            
        except Exception as e:
            print(f"âŒ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            return None, None

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ¨ ê°ì •ë³„ íšŒí™” ìŠ¤íƒ€ì¼ í¬íŠ¸ë ˆì´íŠ¸ ìƒì„± ì‹œì‘!")
    
    # ìƒì„±ê¸° ì´ˆê¸°í™”
    generator = EmotionStylePortrait()
    
    # ì…€ì¹´ íŒŒì¼ ê²½ë¡œ (ì›¹ì—ì„œ ë‹¤ìš´ë¡œë“œí•œ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€)
    selfie_path = "test_sad_face.jpg"
    
    # ê°ì •ë³„ íšŒí™” ìŠ¤íƒ€ì¼ í¬íŠ¸ë ˆì´íŠ¸ ìƒì„±
    result_path, detected_emotion = generator.create_emotion_portrait(selfie_path)
    
    if result_path:
        print(f"\nğŸ‰ ì™„ë£Œ!")
        print(f"   ê°ì§€ëœ ê°ì •: {detected_emotion}")
        print(f"   ê²°ê³¼ ì´ë¯¸ì§€: {result_path}")
        print(f"   ì ìš©ëœ ìŠ¤íƒ€ì¼: {generator.emotion_styles[detected_emotion]['description']}")
    else:
        print("\nâŒ ìƒì„± ì‹¤íŒ¨")

if __name__ == "__main__":
    main()
