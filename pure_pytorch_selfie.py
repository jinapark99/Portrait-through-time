#!/usr/bin/env python3
"""
ðŸš€ ìˆœìˆ˜ PyTorch ì…€ì¹´â†’ì´ˆìƒí™” ë³€í™˜ê¸°
TensorFlow ì—†ì´ PyTorchë§Œ ì‚¬ìš©í•˜ëŠ” ê¹”ë”í•œ ë²„ì „ìž…ë‹ˆë‹¤.
"""

import torch
import argparse
import os
import cv2
import numpy as np
from PIL import Image
from diffusers import StableDiffusionImg2ImgPipeline
import mediapipe as mp

class PurePyTorchSelfieToPortrait:
    def __init__(self, lora_model_path="lora_trained_model/final"):
        """ìˆœìˆ˜ PyTorch ì…€ì¹´â†’ì´ˆìƒí™” ë³€í™˜ê¸° ì´ˆê¸°í™”"""
        print("ðŸš€ ìˆœìˆ˜ PyTorch ì…€ì¹´â†’ì´ˆìƒí™” ë³€í™˜ê¸° ì´ˆê¸°í™” ì¤‘...")
        
        # M1/M2 GPU ì‚¬ìš© ì„¤ì •
        if torch.backends.mps.is_available():
            self.device = "mps"  # Apple Silicon GPU
            print("ðŸŽ Apple Silicon GPU (MPS) ì‚¬ìš©!")
        elif torch.cuda.is_available():
            self.device = "cuda"
            print("ðŸš€ NVIDIA GPU ì‚¬ìš©!")
        else:
            self.device = "cpu"
            print("ðŸ’» CPU ì‚¬ìš©")
        
        print(f"ðŸ“± ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        # MediaPipe Face Detection ì´ˆê¸°í™”
        print("ðŸ‘¤ ì–¼êµ´ ê²€ì¶œ ëª¨ë¸ ë¡œë“œ ì¤‘...")
        self.mp_face_detection = mp.solutions.face_detection
        self.face_detection = self.mp_face_detection.FaceDetection(
            model_selection=1, min_detection_confidence=0.5
        )
        
        print("âœ… ìˆœìˆ˜ PyTorch ë³€í™˜ê¸° ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def detect_face(self, image_path):
        """ì´ë¯¸ì§€ì—ì„œ ì–¼êµ´ ê²€ì¶œ"""
        print(f"ðŸ‘¤ ì–¼êµ´ ê²€ì¶œ ì¤‘: {image_path}")
        
        # ì´ë¯¸ì§€ ì½ê¸°
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"ì´ë¯¸ì§€ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
        
        # RGBë¡œ ë³€í™˜
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # ì–¼êµ´ ê²€ì¶œ
        results = self.face_detection.process(image_rgb)
        
        if not results.detections:
            raise ValueError("ì–¼êµ´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        
        print(f"âœ… {len(results.detections)}ê°œì˜ ì–¼êµ´ ë°œê²¬!")
        return image, image_rgb, results.detections
    
    def analyze_emotion_simple(self, image_path):
        """ê°„ë‹¨í•œ ê°ì • ë¶„ì„ (ìƒ‰ìƒ ê¸°ë°˜)"""
        print("ðŸ” ê°„ë‹¨í•œ ê°ì • ë¶„ì„ ì¤‘...")
        
        try:
            # ì´ë¯¸ì§€ ë¡œë“œ
            image = cv2.imread(image_path)
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # ì–¼êµ´ ê²€ì¶œ
            results = self.face_detection.process(image_rgb)
            
            if results.detections:
                # ì²« ë²ˆì§¸ ì–¼êµ´ ì˜ì—­ ë¶„ì„
                face = results.detections[0]
                bbox = face.location_data.relative_bounding_box
                
                h, w = image_rgb.shape[:2]
                x = int(bbox.xmin * w)
                y = int(bbox.ymin * h)
                width = int(bbox.width * w)
                height = int(bbox.height * h)
                
                # ì–¼êµ´ ì˜ì—­ ì¶”ì¶œ
                face_crop = image_rgb[y:y+height, x:x+width]
                
                # ìƒ‰ìƒ ë¶„ì„
                avg_color = np.mean(face_crop, axis=(0, 1))
                brightness = np.mean(avg_color)
                
                # ê°„ë‹¨í•œ ê°ì • ì¶”ì •
                if brightness > 160:
                    emotion = "joy"
                elif brightness < 120:
                    emotion = "sadness"
                else:
                    emotion = "neutral"
                    
                print(f"ðŸ’­ ë¶„ì„ëœ ê°ì •: {emotion} (ë°ê¸°: {brightness:.1f})")
            else:
                emotion = "joy"  # ê¸°ë³¸ê°’
                print("ðŸ’­ ì–¼êµ´ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ê°’ 'joy' ì‚¬ìš©")
            
            return emotion, {emotion: 0.8}
            
        except Exception as e:
            print(f"âš ï¸ ê°ì • ë¶„ì„ ì‹¤íŒ¨: {e}")
            print("ðŸ’¡ ê¸°ë³¸ê°’ 'joy' ì‚¬ìš©")
            return "joy", {"joy": 0.8}
    
    def get_emotion_prompt(self, emotion):
        """ê°ì •ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        emotion_prompts = {
            'joy': "a joyful portrait, happy expression, warm smile, bright eyes, cheerful, beautiful lighting, vibrant colors, masterpiece, high quality",
            'sadness': "a melancholic portrait, sad expression, contemplative gaze, gentle mood, soft lighting, muted colors, artistic, high quality",
            'anger': "a powerful portrait, intense expression, strong features, dramatic lighting, bold colors, dynamic, high quality",
            'fear': "a thoughtful portrait, cautious expression, mysterious atmosphere, subtle lighting, cool tones, introspective, high quality",
            'surprise': "an expressive portrait, surprised expression, wide eyes, dynamic pose, bright lighting, energetic, high quality",
            'disgust': "a dignified portrait, composed expression, refined features, elegant lighting, sophisticated, classical, high quality",
            'neutral': "a serene portrait, calm expression, peaceful atmosphere, balanced lighting, natural colors, tranquil, high quality",
            'love': "a tender portrait, gentle expression, warm atmosphere, caring eyes, romantic lighting, soft colors, intimate, high quality"
        }
        return emotion_prompts.get(emotion, emotion_prompts['joy'])
    
    def create_portrait_pure(self, image_path, output_path="my_portrait_pure.png"):
        """
        ìˆœìˆ˜ PyTorch ì´ˆìƒí™” ìƒì„±
        """
        print(f"\nðŸŽ¨ ìˆœìˆ˜ PyTorch ì´ˆìƒí™” ìƒì„± ì‹œìž‘!")
        print(f"ðŸ“¸ ìž…ë ¥ ì´ë¯¸ì§€: {image_path}")
        
        # 1. ê°ì • ë¶„ì„
        emotion, emotion_scores = self.analyze_emotion_simple(image_path)
        
        # 2. í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self.get_emotion_prompt(emotion)
        print(f"ðŸ“ ìƒì„± í”„ë¡¬í”„íŠ¸: {prompt}")
        
        # 3. PyTorch íŒŒì´í”„ë¼ì¸ ë¡œë“œ
        print("ðŸŽ¨ PyTorch íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì¤‘...")
        
        # íŒŒì´í”„ë¼ì¸ ë¡œë“œ
        pipe = StableDiffusionImg2ImgPipeline.from_pretrained(
            "runwayml/stable-diffusion-v1-5",
            torch_dtype=torch.float16 if self.device in ["mps", "cuda"] else torch.float32,
            safety_checker=None,
            requires_safety_checker=False
        )
        
        # LoRA ëª¨ë¸ ë¡œë“œ
        lora_path = "lora_trained_model/final"
        if os.path.exists(lora_path):
            print(f"ðŸŽ­ LoRA ëª¨ë¸ ë¡œë“œ: {lora_path}")
            pipe.load_lora_weights(lora_path)
        
        # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        pipe = pipe.to(self.device)
        print(f"ðŸš€ {self.device.upper()}ë¡œ ì´ë™ ì™„ë£Œ!")
        
        # ì›ë³¸ ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬
        init_image = Image.open(image_path).convert("RGB")
        init_image = init_image.resize((512, 512))
        
        # ì´ë¯¸ì§€ ìƒì„±
        print("ðŸ–¼ï¸ ì´ë¯¸ì§€ ìƒì„± ì¤‘... (ì§„í–‰ë¥  í‘œì‹œ)")
        result = pipe(
            prompt=prompt,
            image=init_image,
            strength=0.7,  # ì›ë³¸ ì–¼êµ´ êµ¬ì¡° ìœ ì§€
            guidance_scale=7.5,
            num_inference_steps=20,  # ë¹ ë¥¸ ìƒì„±
            generator=torch.Generator(device=self.device)
        )
        
        # ì €ìž¥
        result.images[0].save(output_path)
        print(f"âœ… ì´ˆìƒí™” ì €ìž¥ ì™„ë£Œ: {output_path}")
        
        return result.images[0], emotion, emotion_scores


def main():
    parser = argparse.ArgumentParser(description="ìˆœìˆ˜ PyTorch ì…€ì¹´â†’ì´ˆìƒí™” ë³€í™˜")
    parser.add_argument("--input", type=str, required=True, help="ìž…ë ¥ ì…€ì¹´ ì´ë¯¸ì§€ ê²½ë¡œ")
    parser.add_argument("--output", type=str, default="my_portrait_pure.png", help="ì¶œë ¥ íŒŒì¼ëª…")
    parser.add_argument("--lora-path", type=str, default="lora_trained_model/final", 
                       help="LoRA ëª¨ë¸ ê²½ë¡œ")
    
    args = parser.parse_args()
    
    # ìž…ë ¥ íŒŒì¼ í™•ì¸
    if not os.path.exists(args.input):
        print(f"âŒ ìž…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.input}")
        return
    
    # ë³€í™˜ê¸° ì´ˆê¸°í™”
    converter = PurePyTorchSelfieToPortrait(lora_model_path=args.lora_path)
    
    # ì´ˆìƒí™” ìƒì„±
    image, emotion, scores = converter.create_portrait_pure(
        args.input, 
        args.output
    )
    
    print(f"\nðŸŽ‰ ì™„ë£Œ!")
    print(f"ðŸŽ­ ê°ì •: {emotion}")
    print(f"ðŸ“Š ê°ì • ì ìˆ˜: {scores}")
    print(f"ðŸ’¾ ì €ìž¥ ìœ„ì¹˜: {args.output}")

if __name__ == "__main__":
    main()





