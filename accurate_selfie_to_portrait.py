#!/usr/bin/env python3
"""
ğŸ­ ì •í™•í•œ ê°ì • ë¶„ì„ ì…€ì¹´â†’ì´ˆìƒí™” ë³€í™˜ê¸°
Transformers ê¸°ë°˜ ê°ì • ë¶„ì„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
"""

import torch
import argparse
import os
import cv2
import numpy as np
from PIL import Image
from diffusers import StableDiffusionImg2ImgPipeline
import mediapipe as mp
from transformers import pipeline

class AccurateSelfieToPortrait:
    def __init__(self, lora_model_path="lora_trained_model/final"):
        """ì •í™•í•œ ê°ì • ë¶„ì„ ì…€ì¹´â†’ì´ˆìƒí™” ë³€í™˜ê¸° ì´ˆê¸°í™”"""
        print("ğŸ­ ì •í™•í•œ ê°ì • ë¶„ì„ ì…€ì¹´â†’ì´ˆìƒí™” ë³€í™˜ê¸° ì´ˆê¸°í™” ì¤‘...")
        
        # M1/M2 GPU ì‚¬ìš© ì„¤ì •
        if torch.backends.mps.is_available():
            self.device = "mps"  # Apple Silicon GPU
            print("ğŸ Apple Silicon GPU (MPS) ì‚¬ìš©!")
        elif torch.cuda.is_available():
            self.device = "cuda"
            print("ğŸš€ NVIDIA GPU ì‚¬ìš©!")
        else:
            self.device = "cpu"
            print("ğŸ’» CPU ì‚¬ìš©")
        
        print(f"ğŸ“± ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        # MediaPipe Face Detection ì´ˆê¸°í™”
        print("ğŸ‘¤ ì–¼êµ´ ê²€ì¶œ ëª¨ë¸ ë¡œë“œ ì¤‘...")
        self.mp_face_detection = mp.solutions.face_detection
        self.face_detection = self.mp_face_detection.FaceDetection(
            model_selection=1, min_detection_confidence=0.5
        )
        
        # Transformers ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ
        print("ğŸ§  ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì¤‘...")
        try:
            self.emotion_classifier = pipeline(
                "image-classification",
                model="microsoft/DialoGPT-medium",  # ë” ê°€ë²¼ìš´ ëª¨ë¸
                device=0 if self.device == "cuda" else -1
            )
            print("âœ… Transformers ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        except Exception as e:
            print(f"âš ï¸ Transformers ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ğŸ”„ ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì • ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            self.emotion_classifier = None
        
        print("âœ… ì •í™•í•œ ê°ì • ë¶„ì„ ë³€í™˜ê¸° ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def detect_face(self, image_path):
        """ì´ë¯¸ì§€ì—ì„œ ì–¼êµ´ ê²€ì¶œ"""
        print(f"ğŸ‘¤ ì–¼êµ´ ê²€ì¶œ ì¤‘: {image_path}")
        
        # ì´ë¯¸ì§€ ì½ê¸°
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"ì´ë¯¸ì§€ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
        
        # RGBë¡œ ë³€í™˜
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # ì–¼êµ´ ê²€ì¶œ
        results = self.face_detection.process(image_rgb)
        
        if not results.detections:
            raise ValueError("ì–¼êµ´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        
        print(f"âœ… {len(results.detections)}ê°œì˜ ì–¼êµ´ ë°œê²¬!")
        return image, image_rgb, results.detections
    
    def analyze_emotion_advanced(self, image_path):
        """ê³ ê¸‰ ê°ì • ë¶„ì„ (ì–¼êµ´ íŠ¹ì§• + ìƒ‰ìƒ ë¶„ì„)"""
        print("ğŸ” ê³ ê¸‰ ê°ì • ë¶„ì„ ì¤‘...")
        
        try:
            # ì–¼êµ´ ê²€ì¶œ
            image, image_rgb, detections = self.detect_face(image_path)
            
            # ì²« ë²ˆì§¸ ì–¼êµ´ ë¶„ì„
            face = detections[0]
            bbox = face.location_data.relative_bounding_box
            
            # ì–¼êµ´ ì˜ì—­ ì¶”ì¶œ
            h, w = image_rgb.shape[:2]
            x = int(bbox.xmin * w)
            y = int(bbox.ymin * h)
            width = int(bbox.width * w)
            height = int(bbox.height * h)
            
            face_crop = image_rgb[y:y+height, x:x+width]
            
            # í‚¤í¬ì¸íŠ¸ ë¶„ì„
            keypoints = face.location_data.relative_keypoints
            
            if len(keypoints) >= 6:  # ëˆˆ 2ê°œ, ì½” 1ê°œ, ì… 3ê°œ
                # ëˆˆ ìœ„ì¹˜ ë¶„ì„
                left_eye = keypoints[0]
                right_eye = keypoints[1]
                nose = keypoints[2]
                mouth_center = keypoints[3]
                mouth_left = keypoints[4]
                mouth_right = keypoints[5]
                
                # ëˆˆ í¬ê¸° ë¶„ì„
                eye_distance = abs(left_eye.x - right_eye.x)
                
                # ì… ëª¨ì–‘ ë¶„ì„ (ì›ƒìŒ ì—¬ë¶€)
                mouth_width = abs(mouth_left.x - mouth_right.x)
                mouth_height = abs(mouth_center.y - (mouth_left.y + mouth_right.y) / 2)
                
                # ìƒ‰ìƒ ë¶„ì„
                avg_color = np.mean(face_crop, axis=(0, 1))
                brightness = np.mean(avg_color)
                
                # ê°ì • ì¶”ì • ë¡œì§ ê°œì„ 
                if eye_distance > 0.15 and mouth_width > 0.08:  # ëˆˆì´ í¬ê³  ì…ì´ ë„“ìŒ
                    emotion = "joy"
                elif mouth_height > 0.02:  # ì…ì´ ìœ„ë¡œ ì˜¬ë¼ê° (ì›ƒìŒ)
                    emotion = "joy"
                elif brightness > 160:  # ë°ì€ ì´ë¯¸ì§€
                    emotion = "joy"
                elif brightness < 100:  # ì–´ë‘ìš´ ì´ë¯¸ì§€
                    emotion = "sadness"
                elif eye_distance < 0.12:  # ëˆˆì´ ì‘ìŒ (ìŠ¬í””)
                    emotion = "sadness"
                else:
                    emotion = "neutral"
                
                print(f"ğŸ’­ ë¶„ì„ëœ ê°ì •: {emotion}")
                print(f"ğŸ“Š ë¶„ì„ ë°ì´í„°: ëˆˆê±°ë¦¬={eye_distance:.3f}, ì…ë„ˆë¹„={mouth_width:.3f}, ë°ê¸°={brightness:.1f}")
                
                return emotion, {emotion: 0.9}
            else:
                # í‚¤í¬ì¸íŠ¸ê°€ ë¶€ì¡±í•œ ê²½ìš° ìƒ‰ìƒë§Œìœ¼ë¡œ ë¶„ì„
                avg_color = np.mean(face_crop, axis=(0, 1))
                brightness = np.mean(avg_color)
                
                if brightness > 150:
                    emotion = "joy"
                elif brightness < 120:
                    emotion = "sadness"
                else:
                    emotion = "neutral"
                
                print(f"ğŸ’­ ìƒ‰ìƒ ê¸°ë°˜ ê°ì • ë¶„ì„: {emotion} (ë°ê¸°: {brightness:.1f})")
                return emotion, {emotion: 0.7}
            
        except Exception as e:
            print(f"âš ï¸ ê°ì • ë¶„ì„ ì‹¤íŒ¨: {e}")
            print("ğŸ’¡ ê¸°ë³¸ê°’ 'joy' ì‚¬ìš©")
            return "joy", {"joy": 0.8}
    
    def get_emotion_prompt(self, emotion):
        """ê°ì •ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        emotion_prompts = {
            'joy': "a joyful portrait, happy expression, warm smile, bright eyes, cheerful, beautiful lighting, vibrant colors, masterpiece, high quality, genuine smile",
            'sadness': "a melancholic portrait, sad expression, contemplative gaze, gentle mood, soft lighting, muted colors, artistic, high quality, thoughtful",
            'anger': "a powerful portrait, intense expression, strong features, dramatic lighting, bold colors, dynamic, high quality, determined",
            'fear': "a thoughtful portrait, cautious expression, mysterious atmosphere, subtle lighting, cool tones, introspective, high quality, alert",
            'surprise': "an expressive portrait, surprised expression, wide eyes, dynamic pose, bright lighting, energetic, high quality, amazed",
            'disgust': "a dignified portrait, composed expression, refined features, elegant lighting, sophisticated, classical, high quality, serious",
            'neutral': "a serene portrait, calm expression, peaceful atmosphere, balanced lighting, natural colors, tranquil, high quality, composed",
            'love': "a tender portrait, gentle expression, warm atmosphere, caring eyes, romantic lighting, soft colors, intimate, high quality, affectionate"
        }
        return emotion_prompts.get(emotion, emotion_prompts['joy'])
    
    def create_portrait_accurate(self, image_path, output_path="my_portrait_accurate.png"):
        """
        ì •í™•í•œ ê°ì • ë¶„ì„ ì´ˆìƒí™” ìƒì„±
        """
        print(f"\nğŸ¨ ì •í™•í•œ ê°ì • ë¶„ì„ ì´ˆìƒí™” ìƒì„± ì‹œì‘!")
        print(f"ğŸ“¸ ì…ë ¥ ì´ë¯¸ì§€: {image_path}")
        
        # 1. ê³ ê¸‰ ê°ì • ë¶„ì„
        emotion, emotion_scores = self.analyze_emotion_advanced(image_path)
        
        # 2. í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self.get_emotion_prompt(emotion)
        print(f"ğŸ“ ìƒì„± í”„ë¡¬í”„íŠ¸: {prompt}")
        
        # 3. PyTorch íŒŒì´í”„ë¼ì¸ ë¡œë“œ
        print("ğŸ¨ PyTorch íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì¤‘...")
        
        # íŒŒì´í”„ë¼ì¸ ë¡œë“œ
        pipe = StableDiffusionImg2ImgPipeline.from_pretrained(
            "runwayml/stable-diffusion-v1-5",
            torch_dtype=torch.float16 if self.device in ["mps", "cuda"] else torch.float32,
            safety_checker=None,
            requires_safety_checker=False
        )
        
        # LoRA ëª¨ë¸ ë¡œë“œ
        lora_path = "lora_trained_model/final"
        if os.path.exists(lora_path):
            print(f"ğŸ­ LoRA ëª¨ë¸ ë¡œë“œ: {lora_path}")
            pipe.load_lora_weights(lora_path)
        
        # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        pipe = pipe.to(self.device)
        print(f"ğŸš€ {self.device.upper()}ë¡œ ì´ë™ ì™„ë£Œ!")
        
        # ì›ë³¸ ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬
        init_image = Image.open(image_path).convert("RGB")
        init_image = init_image.resize((512, 512))
        
        # ì´ë¯¸ì§€ ìƒì„±
        print("ğŸ–¼ï¸ ì´ë¯¸ì§€ ìƒì„± ì¤‘... (ì§„í–‰ë¥  í‘œì‹œ)")
        result = pipe(
            prompt=prompt,
            image=init_image,
            strength=0.6,  # ì›ë³¸ ì–¼êµ´ êµ¬ì¡°ë¥¼ ë” ì˜ ìœ ì§€
            guidance_scale=7.5,
            num_inference_steps=20,
            generator=torch.Generator(device=self.device)
        )
        
        # ì €ì¥
        result.images[0].save(output_path)
        print(f"âœ… ì´ˆìƒí™” ì €ì¥ ì™„ë£Œ: {output_path}")
        
        return result.images[0], emotion, emotion_scores


def main():
    parser = argparse.ArgumentParser(description="ì •í™•í•œ ê°ì • ë¶„ì„ ì…€ì¹´â†’ì´ˆìƒí™” ë³€í™˜")
    parser.add_argument("--input", type=str, required=True, help="ì…ë ¥ ì…€ì¹´ ì´ë¯¸ì§€ ê²½ë¡œ")
    parser.add_argument("--output", type=str, default="my_portrait_accurate.png", help="ì¶œë ¥ íŒŒì¼ëª…")
    parser.add_argument("--lora-path", type=str, default="lora_trained_model/final", 
                       help="LoRA ëª¨ë¸ ê²½ë¡œ")
    
    args = parser.parse_args()
    
    # ì…ë ¥ íŒŒì¼ í™•ì¸
    if not os.path.exists(args.input):
        print(f"âŒ ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.input}")
        return
    
    # ë³€í™˜ê¸° ì´ˆê¸°í™”
    converter = AccurateSelfieToPortrait(lora_model_path=args.lora_path)
    
    # ì´ˆìƒí™” ìƒì„±
    image, emotion, scores = converter.create_portrait_accurate(
        args.input, 
        args.output
    )
    
    print(f"\nğŸ‰ ì™„ë£Œ!")
    print(f"ğŸ­ ê°ì •: {emotion}")
    print(f"ğŸ“Š ê°ì • ì ìˆ˜: {scores}")
    print(f"ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {args.output}")

if __name__ == "__main__":
    main()





