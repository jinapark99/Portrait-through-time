#!/usr/bin/env python3
"""
ğŸ¨ ìµœì¢… ì…€ì¹´ â†’ í¬íŠ¸ë ˆì´íŠ¸ ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸
ì‚¬ìš©ìê°€ ë§ˆìŒì— ë“¤ì–´í•˜ëŠ” ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
"""

import torch
import cv2
import numpy as np
from PIL import Image
from diffusers import StableDiffusionImg2ImgPipeline
import mediapipe as mp

class SelfieToPortrait:
    def __init__(self):
        print("ğŸ¨ ì…€ì¹´ â†’ í¬íŠ¸ë ˆì´íŠ¸ ë³€í™˜ê¸° ì´ˆê¸°í™” ì¤‘...")
        
        # M1 GPU ì„¤ì •
        self.device = "mps" if torch.backends.mps.is_available() else "cpu"
        print(f"ğŸ“± ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        # MediaPipe ì–¼êµ´ ê°ì§€ ì´ˆê¸°í™”
        self.mp_face_detection = mp.solutions.face_detection
        self.face_detection = self.mp_face_detection.FaceDetection(
            model_selection=0, min_detection_confidence=0.5
        )
        
        # Stable Diffusion íŒŒì´í”„ë¼ì¸ ë¡œë“œ
        print("ğŸ¨ Stable Diffusion íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì¤‘...")
        self.pipe = StableDiffusionImg2ImgPipeline.from_pretrained(
            "runwayml/stable-diffusion-v1-5",
            torch_dtype=torch.float16 if self.device in ["mps", "cuda"] else torch.float32,
            safety_checker=None,
            requires_safety_checker=False
        )
        self.pipe = self.pipe.to(self.device)
        
        # LoRA ë¡œë“œ
        print("ğŸ­ LoRA ëª¨ë¸ ë¡œë“œ ì¤‘...")
        try:
            self.pipe.load_lora_weights("lora_trained_model/final")
            print("âœ… LoRA ë¡œë“œ ì„±ê³µ!")
        except Exception as e:
            print(f"âŒ LoRA ë¡œë“œ ì‹¤íŒ¨: {e}")
            return
        
        print("âœ… ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def detect_face(self, image):
        """ì–¼êµ´ ê°ì§€"""
        # PILì„ OpenCV í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        
        # ì–¼êµ´ ê°ì§€
        results = self.face_detection.process(cv_image)
        
        if results.detections:
            # ì²« ë²ˆì§¸ ì–¼êµ´ì˜ ë°”ìš´ë”© ë°•ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            detection = results.detections[0]
            bbox = detection.location_data.relative_bounding_box
            
            h, w, _ = cv_image.shape
            x = int(bbox.xmin * w)
            y = int(bbox.ymin * h)
            width = int(bbox.width * w)
            height = int(bbox.height * h)
            
            # ì–¼êµ´ ì˜ì—­ í¬ë¡­
            face_crop = image.crop((x, y, x + width, y + height))
            
            print(f"âœ… ì–¼êµ´ ê°ì§€ ì„±ê³µ: {width}x{height}")
            return face_crop, True
        else:
            print("âŒ ì–¼êµ´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return image, False
    
    def analyze_emotion_simple(self, face_image):
        """ê°„ë‹¨í•œ ê°ì • ë¶„ì„"""
        # ì–¼êµ´ ëœë“œë§ˆí¬ ê°ì§€ (ë” ê´€ëŒ€í•œ ì„¤ì •)
        mp_face_mesh = mp.solutions.face_mesh
        face_mesh = mp_face_mesh.FaceMesh(
            static_image_mode=True,
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.3,  # ì„ê³„ê°’ ë‚®ì¶¤
            min_tracking_confidence=0.3
        )
        
        # PILì„ numpyë¡œ ë³€í™˜
        img_array = np.array(face_image)
        
        # ì–¼êµ´ ëœë“œë§ˆí¬ ê°ì§€
        results = face_mesh.process(img_array)
        
        if results.multi_face_landmarks:
            landmarks = results.multi_face_landmarks[0]
            
            # ì…ê¼¬ë¦¬ ì¢Œí‘œ (ì¸ë±ìŠ¤ 61, 291)
            left_corner = landmarks.landmark[61]
            right_corner = landmarks.landmark[291]
            
            # ì…ê¼¬ë¦¬ ë†’ì´ ì°¨ì´ ê³„ì‚°
            mouth_curve = right_corner.y - left_corner.y
            
            print(f"ğŸ” ì…ê¼¬ë¦¬ ê³¡ì„  ê°’: {mouth_curve:.4f}")
            
            # ê°„ë‹¨í•œ ê°ì • íŒë‹¨ (ì„ê³„ê°’ ì¡°ì •)
            if mouth_curve < -0.005:  # ì…ê¼¬ë¦¬ê°€ ìœ„ë¡œ ì˜¬ë¼ê° (ì„ê³„ê°’ ë‚®ì¶¤)
                emotion = "joyful"
                print("ğŸ˜Š ê°ì • ë¶„ì„: ê¸°ì¨ (ì›ƒëŠ” í‘œì •)")
            else:
                emotion = "neutral"
                print("ğŸ˜ ê°ì • ë¶„ì„: ì¤‘ë¦½")
        else:
            # ëœë“œë§ˆí¬ ê°ì§€ ì‹¤íŒ¨ ì‹œ ì´ë¯¸ì§€ ë°ê¸°ë¡œ ê°ì • ì¶”ì •
            img_array = np.array(face_image)
            brightness = np.mean(img_array)
            
            if brightness > 120:  # ë°ì€ ì´ë¯¸ì§€
                emotion = "joyful"
                print("ğŸ˜Š ê°ì • ë¶„ì„: ê¸°ì¨ (ë°ì€ ì´ë¯¸ì§€ë¡œ ì¶”ì •)")
            else:
                emotion = "neutral"
                print("ğŸ˜ ê°ì • ë¶„ì„: ì¤‘ë¦½ (ëœë“œë§ˆí¬ ê°ì§€ ì‹¤íŒ¨, ë°ê¸° ê¸°ë°˜)")
        
        return emotion
    
    def create_portrait(self, selfie_path):
        """ì…€ì¹´ë¥¼ í¬íŠ¸ë ˆì´íŠ¸ë¡œ ë³€í™˜"""
        print(f"\nğŸ¨ ì…€ì¹´ ì²˜ë¦¬ ì‹œì‘: {selfie_path}")
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        try:
            image = Image.open(selfie_path).convert("RGB")
            print(f"âœ… ì´ë¯¸ì§€ ë¡œë“œ ì„±ê³µ: {image.size}")
        except Exception as e:
            print(f"âŒ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
        
        # ì–¼êµ´ ê°ì§€
        face_crop, face_found = self.detect_face(image)
        
        # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • (512x512)
        input_image = face_crop.resize((512, 512))
        
        # ê°ì • ë¶„ì„
        emotion = self.analyze_emotion_simple(input_image)
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„± (ì‚¬ìš©ìê°€ ë§ˆìŒì— ë“¤ì–´í•˜ëŠ” ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)
        prompt = f"portrait painting, {emotion} expression"
        print(f"ğŸ“ ì‚¬ìš© í”„ë¡¬í”„íŠ¸: {prompt}")
        
        # ì´ë¯¸ì§€ ìƒì„±
        print("ğŸ¨ ì´ë¯¸ì§€ ìƒì„± ì¤‘... (ì•½ 1-2ë¶„ ì†Œìš”)")
        try:
            result = self.pipe(
                prompt=prompt,
                image=input_image,
                strength=0.7,  # ì›ë³¸ ì´ë¯¸ì§€ì™€ì˜ ìœ ì‚¬ë„
                guidance_scale=7.5,  # í”„ë¡¬í”„íŠ¸ ì¤€ìˆ˜ë„
                num_inference_steps=20,  # í’ˆì§ˆì„ ìœ„í•´ ì¦ê°€
                generator=torch.Generator(device=self.device)
            )
            
            # ê²°ê³¼ ì €ì¥
            output_path = f"final_portrait_{emotion}.png"
            result.images[0].save(output_path)
            print(f"âœ… í¬íŠ¸ë ˆì´íŠ¸ ìƒì„± ì™„ë£Œ: {output_path}")
            
            return output_path
            
        except Exception as e:
            print(f"âŒ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            return None

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ¨ ì…€ì¹´ â†’ í¬íŠ¸ë ˆì´íŠ¸ ë³€í™˜ ì‹œì‘!")
    
    # ë³€í™˜ê¸° ì´ˆê¸°í™”
    converter = SelfieToPortrait()
    
    # ì…€ì¹´ íŒŒì¼ ê²½ë¡œ
    selfie_path = "IMG_5241.JPG"
    
    # í¬íŠ¸ë ˆì´íŠ¸ ìƒì„±
    result_path = converter.create_portrait(selfie_path)
    
    if result_path:
        print(f"\nğŸ‰ ì™„ë£Œ! ê²°ê³¼ ì´ë¯¸ì§€: {result_path}")
    else:
        print("\nâŒ ë³€í™˜ ì‹¤íŒ¨")

if __name__ == "__main__":
    main()
